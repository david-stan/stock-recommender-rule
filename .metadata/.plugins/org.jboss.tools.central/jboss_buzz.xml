<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Change data capture with Debezium: A simple how-to, Part 1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/w4gz00yp2kw/" /><category term="Event-Driven" /><category term="Java" /><category term="Kubernetes" /><category term="Microservices" /><category term="Spring Boot" /><category term="Apache Kafka" /><category term="Data Integration" /><category term="debezium" /><category term="kafka connect" /><category term="mysql" /><category term="openshift" /><author><name>Eric Deandrea</name></author><id>https://developers.redhat.com/blog/?p=707777</id><updated>2020-05-08T07:00:16Z</updated><published>2020-05-08T07:00:16Z</published><content type="html">&lt;p&gt;One question always comes up as organizations moving towards being cloud-native, twelve-factor, and stateless: How do you get an organization’s data to these new applications? There are many different patterns out there, but one pattern we will look at today is change data capture. This post is a simple how-to on how to build out a change data capture solution using &lt;a target="_blank" rel="nofollow" href="https://debezium.io"&gt;Debezium&lt;/a&gt; within an &lt;a target="_blank" rel="nofollow" href="https://www.openshift.com/"&gt;OpenShift&lt;/a&gt; environment. Future posts will also add to this and add additional capabilities.&lt;/p&gt; &lt;h2&gt;What is change data capture?&lt;/h2&gt; &lt;p&gt;Another Red Hatter, &lt;a href="https://developers.redhat.com/blog/author/snandaku/"&gt;Sadhana Nandakumar&lt;/a&gt;, sums it up well in &lt;a href="https://developers.redhat.com/blog/2019/09/03/cdc-pipeline-with-red-hat-amq-streams-and-red-hat-fuse/"&gt;one of her posts&lt;/a&gt; around change data capture:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;&amp;#8220;Change data capture (CDC) is a pattern that enables database changes to be monitored and propagated to downstream systems. It is an effective way of enabling reliable microservices integration and solving typical challenges, such as gradually extracting microservices from existing monoliths.&amp;#8221;&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;This pattern lets data become distributed amongst teams, where each team can self-manage their own data while still keeping up-to-date with the original source of data. There are also other patterns, such as &lt;a target="_blank" rel="nofollow" href="https://martinfowler.com/bliki/CQRS.html"&gt;Command Query Responsibility Segregation&lt;/a&gt; (CQRS), which build on this idea.&lt;/p&gt; &lt;h2&gt;What is Debezium?&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://debezium.io"&gt;Debezium&lt;/a&gt; is an open source technology, supported by Red Hat as part of &lt;a href="https://developers.redhat.com/integration/"&gt;Red Hat Integration&lt;/a&gt;, which allows database row-level changes to be captured as events and published to &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/topics/integration/what-is-apache-kafka"&gt;Apache Kafka&lt;/a&gt; topics. Debezium connectors are based on the popular &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/documentation.html#connect"&gt;Apache Kafka Connect API&lt;/a&gt; and can be deployed within &lt;a href="https://developers.redhat.com/products/amq/overview"&gt;Red Hat AMQ Streams&lt;/a&gt; Kafka clusters.&lt;/p&gt; &lt;h2&gt;Application overview&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/edeandrea/summit-lab-spring-music/tree/pipeline"&gt;The application we will use&lt;/a&gt; as our &amp;#8220;monolith&amp;#8221; is a Spring Boot application that uses a MySQL database as its back end. The application itself has adopted the &lt;a target="_blank" rel="nofollow" href="https://martinfowler.com/eaaDev/EventSourcing.html"&gt;Event Sourcing&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://debezium.io/blog/2020/02/10/event-sourcing-vs-cdc/"&gt;Outbox&lt;/a&gt; patterns. This means that the application maintains a separate table within the database consisting of domain events. It is this table that we need to monitor for changes to publish into our Kafka topics. In this example, there is a table called &lt;code&gt;outbox_events&lt;/code&gt; that looks like this:&lt;/p&gt; &lt;pre&gt;+-----------------+--------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-----------------+--------------+------+-----+---------+----------------+ | event_id | bigint(20) | NO | PRI | NULL | auto_increment | | aggregate_id | varchar(255) | NO | | NULL | | | aggregate_type | varchar(255) | NO | | NULL | | | event_timestamp | datetime(6) | NO | | NULL | | | event_type | varchar(255) | NO | | NULL | | | payload | json | YES | | NULL | | +-----------------+--------------+------+-----+---------+----------------+ &lt;/pre&gt; &lt;h2&gt;Setting up the database&lt;/h2&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-04/html/debezium_user_guide/index"&gt;Debezium documentation&lt;/a&gt; has a section on &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-04/html/debezium_user_guide/debezium-connector-for-mysql"&gt;how to set up the Debezium connector to work with a MySQL database&lt;/a&gt;. We need to follow that documentation but in a container-native way since we will run everything on &lt;a target="_blank" rel="nofollow" href="https://www.openshift.com/"&gt;Red Hat OpenShift&lt;/a&gt;. There are many different ways to accomplish this task, but I will describe the way I decided to do it.&lt;/p&gt; &lt;h3&gt;Create an OpenShift project&lt;/h3&gt; &lt;p&gt;The first thing we need to do is log into our OpenShift cluster. In my example, I use OpenShift 4.3. The database setup does not require cluster admin privileges, so any normal user will work fine:&lt;/p&gt; &lt;pre&gt;$ oc login &amp;#60;CLUSTER_API_URL&amp;#62;&lt;/pre&gt; &lt;p&gt;Next, let’s create a project to host our work:&lt;/p&gt; &lt;pre&gt;$ oc new-project debezium-demo&lt;/pre&gt; &lt;h3&gt;Create the MySQL configuration&lt;/h3&gt; &lt;p&gt;From &lt;a target="_blank" rel="nofollow" href="//access.redhat.com/documentation/en-us/red_hat_integration/2020-04/html/debezium_user_guide/debezium-connector-for-mysql#setup-the-mysql-server"&gt;the Debezium documentation on setting up MySQL&lt;/a&gt;, the first thing we need to do is enable the binlog, GTIDs, and query log events. This is typically done in the MySQL configuration file, usually located in &lt;code&gt;/etc/my.cnf&lt;/code&gt;. In our case, we will use &lt;a target="_blank" rel="nofollow" href="https://github.com/sclorg/mysql-container/tree/master/8.0"&gt;Red Hat’s MySQL 8.0 container image&lt;/a&gt;. This image is already deployed in most OpenShift installations in the &lt;code&gt;openshift&lt;/code&gt; namespace under the &lt;code&gt;mysql:8.0&lt;/code&gt; tag. The source of this image comes from &lt;code&gt;registry.redhat.io/rhscl/mysql-80-rhel7:latest&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;According to the &lt;a target="_blank" rel="nofollow" href="https://github.com/sclorg/mysql-container/tree/master/8.0#default-mycnf-file"&gt;container image documentation&lt;/a&gt;, the default configuration file is at &lt;code&gt;/etc/my.cnf&lt;/code&gt;, but there is an environment variable, &lt;code&gt;MYSQL_DEFAULTS_FILE&lt;/code&gt;, that can be used to override its location. MySQL configuration also lets one configuration file include other configuration files, so we will create a new configuration file that first includes the default configuration and then overrides some of that configuration to enable the required Debezium configuration.&lt;/p&gt; &lt;p&gt;We’ll do this by first creating a configuration file containing our configuration. We’ll call this file &lt;code&gt;my-debezium.cnf&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;!include /etc/my.cnf [mysqld] server-id = 223344 server_id = 223344 log_bin = ON binlog_format = ROW binlog_row_image = full binlog_rows_query_log_events = ON expire_logs_days = 10 gtid_mode = ON enforce_gtid_consistency = ON &lt;/pre&gt; &lt;p&gt;Now that our MySQL configuration file is created, let&amp;#8217;s create it as a &lt;code&gt;ConfigMap&lt;/code&gt; within our OpenShift project:&lt;/p&gt; &lt;pre&gt;$ oc create configmap db-config --from-file=my-debezium.cnf&lt;/pre&gt; &lt;h3&gt;Create a MySQL user&lt;/h3&gt; &lt;p&gt;The next part of the Debezium MySQL configuration is to &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-04/html-single/debezium_user_guide/index#create-a-mysql-user-for-cdc_cdc"&gt;create a MySQL user for the connector&lt;/a&gt;. We will follow the same pattern that we did for the configuration by creating a file containing the needed SQL. This &lt;code&gt;initdb.sql&lt;/code&gt; file will create a user with the ID &lt;code&gt;debezium&lt;/code&gt; and password &lt;code&gt;debezium&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;CREATE USER IF NOT EXISTS 'debezium'@'%' IDENTIFIED WITH mysql_native_password BY 'debezium'; GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'debezium'@'%'; FLUSH PRIVILEGES; &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note:&lt;/strong&gt; In a real production environment, we want to choose usernames and passwords more carefully, as well as only allowing the &lt;code&gt;debezium&lt;/code&gt; user access to the tables it will monitor.&lt;/p&gt; &lt;p&gt;Now create a &lt;code&gt;ConfigMap&lt;/code&gt; within our OpenShift project:&lt;/p&gt; &lt;pre&gt;$ oc create configmap db-init --from-file=initdb.sql&lt;/pre&gt; &lt;p&gt;The last piece of the configuration is to create an OpenShift &lt;code&gt;Secret&lt;/code&gt; to hold onto our database credentials. This &lt;code&gt;Secret&lt;/code&gt; will be used by our database as well as the application that connects to the database. For simplicity, we will use &lt;code&gt;music&lt;/code&gt; as our database name, username, password, and admin password:&lt;/p&gt; &lt;pre&gt;$ oc create secret generic db-creds --from-literal=database-name=music --from-literal=database-password=music --from-literal=database-user=music --from-literal=database-admin-password=music &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note (again):&lt;/strong&gt; In a real production environment, we want to choose usernames and passwords more carefully.&lt;/p&gt; &lt;h3&gt;Deploy MySQL&lt;/h3&gt; &lt;p&gt;The last part is to create the database and point it to our two configurations. OpenShift allows us to take the &lt;code&gt;ConfigMap&lt;/code&gt;s we created and mount them as files within the container filesystem. We can then use environment variables to change the behavior of the MySQL container image. Let’s create a descriptor YAML file, &lt;code&gt;mysql.yml&lt;/code&gt;, for our database &lt;code&gt;DeploymentConfig&lt;/code&gt; and &lt;code&gt;Service&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;kind: DeploymentConfig apiVersion: apps.openshift.io/v1 metadata: name: spring-music-db labels: application: spring-music app: spring-music app.kubernetes.io/part-of: spring-music app.openshift.io/runtime: mysql-database spec: replicas: 1 strategy: type: Recreate recreateParams: post: failurePolicy: Abort execNewPod: command: - /bin/sh - '-c' - sleep 10 &amp;#38;&amp;#38; MYSQL_PWD="$MYSQL_ROOT_PASSWORD" $MYSQL_PREFIX/bin/mysql -h $SPRING_MUSIC_DB_SERVICE_HOST -u root &amp;#60; /config/initdb.d/initdb.sql containerName: spring-music-db volumes: - db-init selector: name: spring-music-db template: metadata: name: spring-music-db labels: name: spring-music-db spec: volumes: - name: db-data emptyDir: {} - name: db-init configMap: name: db-init - name: db-config configMap: name: db-config containers: - env: - name: MYSQL_DEFAULTS_FILE value: /config/configdb.d/my-debezium.cnf - name: MYSQL_USER valueFrom: secretKeyRef: name: db-creds key: database-user - name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: db-creds key: database-password - name: MYSQL_DATABASE valueFrom: secretKeyRef: name: db-creds key: database-name - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: db-creds key: database-admin-password name: spring-music-db image: ' ' imagePullPolicy: IfNotPresent volumeMounts: - name: db-data mountPath: /var/lib/mysql/data - name: db-init mountPath: /config/initdb.d - name: db-config mountPath: /config/configdb.d ports: - containerPort: 3306 protocol: TCP livenessProbe: failureThreshold: 3 initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 3306 timeoutSeconds: 1 readinessProbe: exec: command: - /bin/sh - -i - -c - MYSQL_PWD="$MYSQL_PASSWORD" mysql -h 127.0.0.1 -u $MYSQL_USER -D $MYSQL_DATABASE -e 'SELECT 1' failureThreshold: 3 initialDelaySeconds: 5 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: limits: memory: 512Mi securityContext: privileged: false triggers: - type: ConfigChange - type: ImageChange imageChangeParams: automatic: true containerNames: - spring-music-db from: kind: ImageStreamTag name: mysql:8.0 namespace: openshift --- kind: Service apiVersion: v1 metadata: name: spring-music-db labels: application: spring-music app: spring-music annotations: template.openshift.io/expose-uri: mysql://{.spec.clusterIP}:{.spec.ports[?(.name=="mysql")].port} spec: ports: - name: mysql port: 3306 protocol: TCP targetPort: 3306 selector: name: spring-music-db &lt;/pre&gt; &lt;p&gt;From this &lt;code&gt;DeploymentConfig&lt;/code&gt;, you can see that we mount our &lt;code&gt;db-init&lt;/code&gt; and &lt;code&gt;db-config&lt;/code&gt; &lt;code&gt;ConfigMap&lt;/code&gt;s as volumes on the container filesystem inside the &lt;code&gt;/config&lt;/code&gt; directory on lines 72-75:&lt;/p&gt; &lt;pre&gt;volumeMounts: - name: db-data mountPath: /var/lib/mysql/data - name: db-init mountPath: /config/initdb.d - name: db-config mountPath: /config/configdb.d&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;/config/configdb.d/my-debezium.cnf&lt;/code&gt; file is also set as the value for the &lt;code&gt;MYSQL_DEFAULTS_FILE&lt;/code&gt; environment variable on lines 44-45:&lt;/p&gt; &lt;pre&gt;- env: - name: MYSQL_DEFAULTS_FILE value: /config/configdb.d/my-debezium.cnf&lt;/pre&gt; &lt;p&gt;The database initialization script from the &lt;code&gt;db-init&lt;/code&gt; &lt;code&gt;ConfigMap&lt;/code&gt; is executed as a post &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.3/applications/deployments/deployment-strategies.html#deployments-lifecycle-hooks_deployment-strategies"&gt;lifecycle hook&lt;/a&gt; on lines 15-24:&lt;/p&gt; &lt;pre&gt;post: failurePolicy: Abort execNewPod: command: - /bin/sh - '-c' - sleep 10 &amp;#38;&amp;#38; MYSQL_PWD="$MYSQL_ROOT_PASSWORD" $MYSQL_PREFIX/bin/mysql -h $SPRING_MUSIC_DB_SERVICE_HOST -u root &amp;#60; /config/initdb.d/initdb.sql containerName: spring-music-db volumes: - db-init&lt;/pre&gt; &lt;p&gt;Our MySQL instance here is ephemeral, so whenever a new container instance is created the script will execute in a sidecar container within the pod.&lt;/p&gt; &lt;p&gt;Now create the resources and wait for the database pod to start:&lt;/p&gt; &lt;pre&gt;$ oc create -f mysql.yml&lt;/pre&gt; &lt;h2&gt;Starting the application&lt;/h2&gt; &lt;p&gt;Now that our database is up and running we can start the application. Let&amp;#8217;s go to the OpenShift web console and then to the &lt;strong&gt;Developer&lt;/strong&gt; perspective&amp;#8217;s &lt;strong&gt;Topology&lt;/strong&gt; view, as shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_707847" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707847" class="wp-image-707847 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/topology-view-1024x380.png" alt="Navigate to Developer Perspective" width="640" height="238" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/topology-view-1024x380.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/topology-view-300x111.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/topology-view-768x285.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/topology-view.png 1396w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-707847" class="wp-caption-text"&gt;Figure 1: Navigate to Developer Perspective&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Then click the &lt;strong&gt;+Add&lt;/strong&gt; button, followed by the &lt;strong&gt;Container Image&lt;/strong&gt; tile, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_707857" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707857" class="wp-image-707857 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/add-app-container-image-1024x618.png" alt="Add new container image" width="640" height="386" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/add-app-container-image-1024x618.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/add-app-container-image-300x181.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/add-app-container-image-768x464.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/add-app-container-image.png 1259w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-707857" class="wp-caption-text"&gt;Figure 2: Add new container image&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Fill in the image name with &lt;code&gt;quay.io/edeandrea/spring-music:latest&lt;/code&gt; and then click the search button, as shown in Figure 3.&lt;/p&gt; &lt;div id="attachment_707877" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-image-name.png"&gt;&lt;img aria-describedby="caption-attachment-707877" class="wp-image-707877" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-image-name.png" alt="Load container image" width="640" height="535" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-image-name.png 973w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-image-name-300x251.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-image-name-768x642.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707877" class="wp-caption-text"&gt;Figure 3: Load container image&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Then fill out the rest of the information from Figures 4 and 5 below, making sure to add the correct labels and environment variables by clicking the links at the bottom with the sentence &amp;#8220;Click on the names to access advanced options for Routing, Deployment, Scaling, Resource Limits, and Labels.&amp;#8221;&lt;/p&gt; &lt;p&gt;The fields and values should be filled out as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Application:&lt;/strong&gt; spring-music&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt;: spring-music&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deployment Config:&lt;/strong&gt; selected&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Create a route to the application:&lt;/strong&gt; checked&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Labels&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;app.openshift.io/runtime=spring&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Auto deploy when new image is available&lt;/strong&gt;: checked&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auto deploy when deployment configuration changes:&lt;/strong&gt; checked&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Environment Variables&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Add from Config Map or Secret&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;NAME: &lt;/strong&gt;SPRING_DATASOURCE_USERNAME&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VALUE:&lt;/strong&gt; From Secret &lt;strong&gt;db-creds&lt;/strong&gt; field &lt;strong&gt;database-user&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Add from Config Map or Secret&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;NAME: &lt;/strong&gt;SPRING_DATASOURCE_PASSWORD&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VALUE:&lt;/strong&gt; from Secret &lt;strong&gt;db-creds&lt;/strong&gt; field &lt;strong&gt;database-password&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Add Value&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;NAME: &lt;/strong&gt;SPRING_DATASOURCE_URL&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VALUE:&lt;/strong&gt; jdbc:mysql://spring-music-db/music&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;div id="attachment_707887" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment1.png"&gt;&lt;img aria-describedby="caption-attachment-707887" class="wp-image-707887" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment1.png" alt="Complete application details" width="640" height="546" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment1.png 791w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment1-300x256.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment1-768x655.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707887" class="wp-caption-text"&gt;Figure 4: Complete application details, Part 1&lt;/p&gt;&lt;/div&gt; &lt;div id="attachment_707897" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment2.png"&gt;&lt;img aria-describedby="caption-attachment-707897" class="wp-image-707897" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment2.png" alt="Complete application details" width="640" height="473" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment2.png 994w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment2-300x222.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-deployment2-768x568.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707897" class="wp-caption-text"&gt;Figure 5: Complete application details, Part 2&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Once done, click the &lt;strong&gt;Create&lt;/strong&gt; button.&lt;/p&gt; &lt;p&gt;Back in the &lt;strong&gt;Topology&lt;/strong&gt; view, you should see the application spin up. Once it is surrounded by the blue ring, click the route button on the top-right corner of the application icon, as shown in Figure 6.&lt;/p&gt; &lt;div id="attachment_707907" style="width: 424px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707907" class="wp-image-707907 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-route.png" alt="Launch application UI" width="414" height="269" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-route.png 414w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/app-route-300x195.png 300w" sizes="(max-width: 414px) 100vw, 414px" /&gt;&lt;p id="caption-attachment-707907" class="wp-caption-text"&gt;Figure 6: Launch application UI&lt;/p&gt;&lt;/div&gt; &lt;p&gt;This will launch the application. Feel free to play around with it if you&amp;#8217;d like. Try deleting an album.&lt;/p&gt; &lt;h2&gt;Deploy AMQ Streams&lt;/h2&gt; &lt;p&gt;Now that our database and application are up and running let’s deploy our AMQ Streams cluster. First, we need to install the AMQ Streams Operator into the cluster from the OperatorHub. To do this you need cluster admin privileges for your OpenShift cluster. Log in to the web console as a cluster admin, then on the left expand &lt;strong&gt;OperatorHub&lt;/strong&gt;, search for &lt;strong&gt;AMQ Streams&lt;/strong&gt;, and select &lt;strong&gt;Red Hat Integration &amp;#8211; AMQ Streams&lt;/strong&gt;, as shown in Figure 7.&lt;/p&gt; &lt;div id="attachment_707927" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707927" class="wp-image-707927 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install-1024x683.png" alt="Find AMQ Streams operator" width="640" height="427" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install-1024x683.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install-300x200.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install-768x512.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install.png 1050w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-707927" class="wp-caption-text"&gt;Figure 7: Find AMQ Streams Operator&lt;/p&gt;&lt;/div&gt; &lt;p&gt;On the installation screen, click the &lt;strong&gt;Install&lt;/strong&gt; button, as shown in Figure 8.&lt;/p&gt; &lt;div id="attachment_707937" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install2.png"&gt;&lt;img aria-describedby="caption-attachment-707937" class="wp-image-707937" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install2.png" alt="Install AMQ Streams operator" width="640" height="521" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install2.png 889w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install2-300x244.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install2-768x625.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707937" class="wp-caption-text"&gt;Figure 8: Install AMQ Streams Operator&lt;/p&gt;&lt;/div&gt; &lt;p&gt;On the &lt;strong&gt;Create Operator Subscription&lt;/strong&gt; page, leave the defaults and click &lt;strong&gt;Subscribe&lt;/strong&gt;, as shown in Figure 9. This action will install the Operator for all of the projects in the cluster.&lt;/p&gt; &lt;div id="attachment_707957" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707957" class="wp-image-707957" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install3.png" alt="Create operator subscription" width="640" height="533" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install3.png 769w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install3-300x250.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install3-768x639.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-707957" class="wp-caption-text"&gt;Figure 9: Create Operator subscription&lt;/p&gt;&lt;/div&gt; &lt;p&gt;You&amp;#8217;ll then be brought to the &lt;strong&gt;Installed Operators&lt;/strong&gt; screen. Sit tight and wait for the Red &lt;strong&gt;Hat Integration &amp;#8211; AMQ Streams&lt;/strong&gt; Operator to show up with &lt;strong&gt;Succeeded&lt;/strong&gt; status, as shown in Figure 10. It shouldn&amp;#8217;t take more than a minute or two.&lt;/p&gt; &lt;div id="attachment_707967" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707967" class="wp-image-707967 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install4-1024x396.png" alt="Wait for operator to provision" width="640" height="248" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install4-1024x396.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install4-300x116.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install4-768x297.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-operator-install4.png 1257w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-707967" class="wp-caption-text"&gt;Figure 10: Wait for the Operator to provision&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now let&amp;#8217;s create our Kafka cluster. Click on the &lt;strong&gt;Red Hat Integration &amp;#8211; AMQ Streams&lt;/strong&gt; label to get to the main AMQ Streams Operator page. Then under &lt;strong&gt;Provided APIs&lt;/strong&gt;, click the &lt;strong&gt;Create Instance&lt;/strong&gt; label in the &lt;strong&gt;Kafka&lt;/strong&gt; section, as shown in Figure 11.&lt;/p&gt; &lt;div id="attachment_707977" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-707977" class="wp-image-707977" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install1.png" alt="Create Kafka instance" width="640" height="474" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install1.png 977w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install1-300x222.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install1-768x568.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-707977" class="wp-caption-text"&gt;Figure 11: Create Kafka instance&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The &lt;strong&gt;Create Kafka YAML&lt;/strong&gt; editor will then come up. Remove everything that&amp;#8217;s there, paste in the following, and click the &lt;b&gt;Create&lt;/b&gt; button at the bottom of the screen:&lt;/p&gt; &lt;pre&gt;kind: Kafka apiVersion: kafka.strimzi.io/v1beta1 metadata: name: db-events namespace: debezium-demo labels: app: spring-music-cdc template: spring-music-cdc app.kubernetes.io/part-of: spring-music-cdc spec: kafka: replicas: 3 listeners: plain: {} jvmOptions: gcLoggingEnabled: false config: auto.create.topics.enable: "true" num.partitions: 1 offsets.topic.replication.factor: 3 default.replication.factor: 3 transaction.state.log.replication.factor: 3 transaction.state.log.min.isr: 2 storage: type: persistent-claim size: 100Gi deleteClaim: true template: statefulset: metadata: labels: app.kubernetes.io/part-of: spring-music-cdc app: spring-music-cdc template: spring-music-cdc annotations: app.openshift.io/connects-to: db-events-zookeeper zookeeper: replicas: 3 storage: type: persistent-claim size: 100Gi deleteClaim: true template: statefulset: metadata: labels: app.kubernetes.io/part-of: spring-music-cdc app: spring-music-cdc template: spring-music-cdc entityOperator: topicOperator: {} userOperator: {} &lt;/pre&gt; &lt;p&gt;This action will deploy a three-node Kafka cluster along with a three-node Zookeeper cluster. It will also turn down the JVM&amp;#8217;s garbage collection logging so that if we need to look at the logs in any of the Kafka broker pods they won’t be polluted with tons of garbage collection debug logs. Both the Kafka and Zookeeper brokers are backed by persistent storage, so the data will survive a broker and cluster restart.&lt;/p&gt; &lt;p&gt;Wait a few minutes for OpenShift to spin everything up. You can switch to the OpenShift &lt;strong&gt;Developer&lt;/strong&gt; perspective’s &lt;strong&gt;Topology&lt;/strong&gt; view by clicking what is shown in Figure 12.&lt;/p&gt; &lt;div id="attachment_708017" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708017" class="wp-image-708017 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-switch-to-dev-1024x478.png" alt="Switch to developer perspective" width="640" height="299" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-switch-to-dev-1024x478.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-switch-to-dev-300x140.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-switch-to-dev-768x358.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-switch-to-dev.png 1372w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708017" class="wp-caption-text"&gt;Figure 12: Switch to developer perspective&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Once the &lt;strong&gt;db-events-entity-operator, db-events-kafka, &lt;/strong&gt;and&lt;strong&gt; db-events-zookeeper&lt;/strong&gt; items all show up with a blue ring around them, as shown in Figure 13, you are done.&lt;/p&gt; &lt;div id="attachment_708027" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708027" class="wp-image-708027 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install-finished-1024x565.png" alt="Wait for Kafka deployment" width="640" height="353" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install-finished-1024x565.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install-finished-300x165.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install-finished-768x423.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/amq-streams-install-finished.png 1090w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708027" class="wp-caption-text"&gt;Figure 13: Wait for Kafka deployment&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Deploy Kafka Connect&lt;/h2&gt; &lt;p&gt;Debezium runs inside a Kafka Connect cluster, so that means we need a container image with both Kafka Connect and the Debezium libraries together. The easiest way to do this is to &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.6/html-single/using_amq_streams_on_openshift/index#creating-new-image-from-base-str"&gt;create your own container image from the Kafka Connect base image&lt;/a&gt;. What follows are the steps needed to do this. I also already created an image you can use, so feel free to skip this sub-section if you would like and use the image at &lt;code&gt;quay.io/edeandrea/kafka-connect-debezium-mysql:amq-streams-1.4.0-dbz-1.1.0.Final&lt;/code&gt; instead.&lt;/p&gt; &lt;h3&gt;Building your own Kafka Connect image&lt;/h3&gt; &lt;p&gt;To build your own Kafka Connect image:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a directory on your local computer (i.e., &lt;code&gt;debezium-connect-image&lt;/code&gt;) and then &lt;code&gt;cd&lt;/code&gt; into that directory.&lt;/li&gt; &lt;li&gt;Create a directory inside called &lt;code&gt;plugins&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Download the Debezium MySQL connector from the &lt;a href="https://debezium.io/releases/"&gt;Debezium Releases page&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note:&lt;/strong&gt; This post was written using the &lt;code&gt;1.1.0.Final&lt;/code&gt; version of the MySQL connector, but whatever the latest version listed should do fine.&lt;/p&gt; &lt;ol start="4"&gt; &lt;li&gt;Unpackage the downloaded file into the &lt;code&gt;plugins&lt;/code&gt; directory.&lt;/li&gt; &lt;li&gt;Create a &lt;code&gt;Dockerfile&lt;/code&gt; at the root (i.e., &lt;code&gt;debezium-connect-image&lt;/code&gt;) directory with the following contents (you&amp;#8217;ll need an account on &lt;code&gt;registry.redhat.io&lt;/code&gt; and to log into the registry on your machine in order to pull the AMQ Streams image): &lt;pre&gt;FROM registry.redhat.io/amq7/amq-streams-kafka-24-rhel7:1.4.0 USER root:root COPY ./plugins/ /opt/kafka/plugins USER jboss:jboss&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Your directory tree should now look like what&amp;#8217;s shown in Figure 14. &lt;p&gt;&lt;div id="attachment_708127" style="width: 591px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708127" class="wp-image-708127 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/debezium-filesystem-layout.png" alt="Contents of Kafka Connect image" width="581" height="383" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/debezium-filesystem-layout.png 581w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/debezium-filesystem-layout-300x198.png 300w" sizes="(max-width: 581px) 100vw, 581px" /&gt;&lt;p id="caption-attachment-708127" class="wp-caption-text"&gt;Figure 14: Contents of Kafka Connect image&lt;/p&gt;&lt;/div&gt;&lt;/li&gt; &lt;li&gt;Build or tag the image using your favorite tool (i.e., Docker/Buildah/etc.) and push it to your registry of choice.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Create Kafka Connect credentials&lt;/h3&gt; &lt;p&gt;Before we create the KafkaConnect cluster there is one small thing we need to take care of. The Debezium connector requires a connection to the database. Rather than hard-coding the credentials into the configuration, let’s instead create an OpenShift &lt;code&gt;Secret&lt;/code&gt; that contains credentials that can then be mounted into the &lt;code&gt;KafkaConnect&lt;/code&gt; pods.&lt;/p&gt; &lt;p&gt;On your local filesystem, create a file called &lt;code&gt;connector.properties&lt;/code&gt;. The contents of this file should be:&lt;/p&gt; &lt;pre&gt;dbUsername=debezium dbPassword=debezium &lt;/pre&gt; &lt;p&gt;Now let’s create the OpenShift &lt;code&gt;Secret&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ oc create secret generic db-connector-creds --from-file=connector.properties&lt;/pre&gt; &lt;h3&gt;Deploy the Kafka Connect image&lt;/h3&gt; &lt;p&gt;Back in the OpenShift console go back to the &lt;strong&gt;Administrator&lt;/strong&gt; perspective and go into &lt;b&gt;Installed Operators&lt;/b&gt;, then click on the &lt;b&gt;Red Hat Integration &amp;#8211; AMQ Streams operator&lt;/b&gt;, as shown in Figure 15.&lt;/p&gt; &lt;div id="attachment_708147" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708147" class="wp-image-708147 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-1024x378.png" alt="Installed operators" width="640" height="236" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-1024x378.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-300x111.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-768x284.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams.png 1288w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708147" class="wp-caption-text"&gt;Figure 15: Installed Operators&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Then under &lt;strong&gt;Provided APIs&lt;/strong&gt;, click the &lt;strong&gt;Create Instance&lt;/strong&gt; label in the &lt;strong&gt;Kafka Connect&lt;/strong&gt; section, as shown in Figure 16.&lt;/p&gt; &lt;div id="attachment_708157" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708157" class="wp-image-708157 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-install1-1024x425.png" alt="Create Kafka Connect instance" width="640" height="266" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-install1-1024x425.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-install1-300x124.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-install1-768x319.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-install1.png 1256w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708157" class="wp-caption-text"&gt;Figure 16: Create Kafka Connect instance&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The &lt;strong&gt;Create KafkaConnect YAML&lt;/strong&gt; editor will then come up. Remove everything that&amp;#8217;s there, paste in the following, and click the &lt;b&gt;Create&lt;/b&gt; button at the bottom of the screen:&lt;/p&gt; &lt;pre&gt;kind: KafkaConnect apiVersion: kafka.strimzi.io/v1beta1 metadata: name: db-events namespace: debezium-demo labels: app: spring-music-cdc template: spring-music-cdc annotations: strimzi.io/use-connector-resources: "true" spec: replicas: 1 image: "quay.io/edeandrea/kafka-connect-debezium-mysql:amq-streams-1.4.0-dbz-1.1.0.Final" bootstrapServers: "db-events-kafka-bootstrap:9092" jvmOptions: gcLoggingEnabled: false config: group.id: spring-music-db offset.storage.topic: spring-music-db-offsets config.storage.topic: spring-music-db-configs status.storage.topic: spring-music-db-status config.storage.replication.factor: 1 offset.storage.replication.factor: 1 status.storage.replication.factor: 1 config.providers: file config.providers.file.class: org.apache.kafka.common.config.provider.FileConfigProvider externalConfiguration: volumes: - name: connector-config secret: secretName: db-connector-creds template: deployment: metadata: labels: app: spring-music-cdc app.kubernetes.io/part-of: spring-music-cdc template: spring-music-cdc annotations: app.openshift.io/connects-to: db-events-kafka,spring-music-db &lt;/pre&gt; &lt;p&gt;This action will deploy a one-node KafkaConnect cluster. It will also turn down the JVM&amp;#8217;s garbage collection logging so that if we need to look at the logs in any of the &lt;code&gt;KafkaConnect&lt;/code&gt; pods, they won’t be polluted with tons of garbage collection debug logs.&lt;/p&gt; &lt;p&gt;As you can see from this configuration, we use the &lt;code&gt;quay.io/edeandrea/kafka-connect-debezium-mysql:amq-streams-1.4.0-dbz-1.1.0.Final&lt;/code&gt; image on line 13:&lt;/p&gt; &lt;pre&gt;image: "quay.io/edeandrea/kafka-connect-debezium-mysql:amq-streams-1.4.0-dbz-1.1.0.Final"&lt;/pre&gt; &lt;p&gt;Then we tell the &lt;code&gt;KafkaConnect&lt;/code&gt; cluster to connect to the &lt;code&gt;db-events-kafka-bootstrap:9092&lt;/code&gt; bootstrap server on line 14:&lt;/p&gt; &lt;pre&gt;bootstrapServers: "db-events-kafka-bootstrap:9092"&lt;/pre&gt; &lt;p&gt;We’ve also added some &lt;code&gt;externalConfiguration&lt;/code&gt;, which tells the &lt;code&gt;KafkaConnect&lt;/code&gt; container to mount the secret named &lt;code&gt;db-connector-creds&lt;/code&gt; into the directory &lt;code&gt;/opt/kafka/external-configuration/connector-config&lt;/code&gt; within the running container (lines 27-31):&lt;/p&gt; &lt;pre&gt;externalConfiguration: volumes: - name: connector-config secret: secretName: db-connector-creds&lt;/pre&gt; &lt;p&gt;If you go back to the OpenShift &lt;strong&gt;Developer&lt;/strong&gt; perspective’s &lt;strong&gt;Topology&lt;/strong&gt; view, you should now see the &lt;strong&gt;db-events-connect&lt;/strong&gt; deployment with one replica available, as shown in Figure 17. It might take a few minutes for it to become available.&lt;/p&gt; &lt;div id="attachment_708177" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-available.png"&gt;&lt;img aria-describedby="caption-attachment-708177" class="wp-image-708177" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-available.png" alt="Wait for Kafka Connect to become available" width="640" height="432" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-available.png 1018w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-available-300x202.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connect-available-768x518.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-708177" class="wp-caption-text"&gt;Figure 17: Wait for Kafka Connect to become available&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Deploy the Debezium connector&lt;/h2&gt; &lt;p&gt;Now that our Kafka Connect cluster is up and running we can deploy our Debezium connector configuration into it. Back in the OpenShift console, go back to the &lt;strong&gt;Administrator&lt;/strong&gt; perspective, then &lt;b&gt;Installed Operators&lt;/b&gt;, and then click the &lt;b&gt;Red Hat Integration &amp;#8211; AMQ Streams operator&lt;/b&gt;, as shown in Figure 18.&lt;/p&gt; &lt;div id="attachment_708147" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708147" class="wp-image-708147 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-1024x378.png" alt="Installed operators" width="640" height="236" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-1024x378.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-300x111.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams-768x284.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/go-back-to-amq-streams.png 1288w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708147" class="wp-caption-text"&gt;Figure 18: Installed Operators&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Then under &lt;strong&gt;Provided APIs&lt;/strong&gt;, click the &lt;strong&gt;Create Instance&lt;/strong&gt; label in the &lt;strong&gt;Kafka Connector&lt;/strong&gt; section, as shown in Figure 19.&lt;/p&gt; &lt;div id="attachment_708187" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708187" class="wp-image-708187 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connector-install1-795x1024.png" alt="Create Kafka Connector instance" width="640" height="824" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connector-install1-795x1024.png 795w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connector-install1-233x300.png 233w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connector-install1-768x990.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-connector-install1.png 987w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708187" class="wp-caption-text"&gt;Figure 19: Create Kafka Connector instance&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The &lt;strong&gt;Create KafkaConnector YAML&lt;/strong&gt; editor will then come up. Remove everything that&amp;#8217;s there, paste in the following, and click the &lt;b&gt;Create&lt;/b&gt; button at the bottom of the screen. This action will deploy the connector configuration into the Kafka Connect cluster and start the connector:&lt;/p&gt; &lt;pre&gt;kind: KafkaConnector apiVersion: kafka.strimzi.io/v1alpha1 metadata: name: db-events namespace: debezium-demo labels: app: spring-music-cdc strimzi.io/cluster: db-events spec: class: io.debezium.connector.mysql.MySqlConnector tasksMax: 1 config: database.hostname: spring-music-db database.port: 3306 database.user: "${file:/opt/kafka/external-configuration/connector-config/connector.properties:dbUsername}" database.password: "${file:/opt/kafka/external-configuration/connector-config/connector.properties:dbPassword}" database.dbname: music database.server.name: spring-music database.server.id: 223344 database.whitelist: music database.allowPublicKeyRetrieval: true database.history.kafka.bootstrap.servers: db-events-kafka-bootstrap:9092 database.history.kafka.topic: dbhistory.music table.whitelist: music.outbox_events tombstones.on.delete : false transforms: outbox transforms.outbox.type: io.debezium.transforms.outbox.EventRouter transforms.outbox.route.topic.replacement: "outbox.${routedByValue}.events" transforms.outbox.table.field.event.id: event_id transforms.outbox.table.field.event.key: aggregate_id transforms.outbox.table.field.event.timestamp: event_timestamp transforms.outbox.table.field.event.type: event_type transforms.outbox.table.field.event.payload.id: aggregate_id transforms.outbox.route.by.field: aggregate_type transforms.outbox.table.fields.additional.placement: "event_id:envelope:eventId,event_timestamp:envelope:eventTimestamp,aggregate_id:envelope:aggregateId,aggregate_type:envelope:aggregateType" &lt;/pre&gt; &lt;p&gt;This configuration provides lots of information. You’ll notice that the database username and password are injected into the configuration via the &lt;code&gt;connector.properties&lt;/code&gt; file stored in our OpenShift &lt;code&gt;Secret&lt;/code&gt; on lines 15-16:&lt;/p&gt; &lt;pre&gt;database.user: "${file:/opt/kafka/external-configuration/connector-config/connector.properties:dbUsername}" database.password: "${file:/opt/kafka/external-configuration/connector-config/connector.properties:dbPassword}"&lt;/pre&gt; &lt;p&gt;The configuration also instructs Debezium as to which topic to place events on (line 28):&lt;/p&gt; &lt;pre&gt;transforms.outbox.route.topic.replacement: "outbox.${routedByValue}.events"&lt;/pre&gt; &lt;p&gt;Debezium supports placing all events on a single topic or using a derived routing key to decide the topic. In our case, our application only deals with a single type of domain for its events. For our application, all of the events are stored in the &lt;code&gt;outbox.Album.events&lt;/code&gt; topic.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note:&lt;/strong&gt; If our application worked with more than one kind of domain event that might be unrelated to another, it might make sense to place each domain’s events into different topics.&lt;/p&gt; &lt;p&gt;Debezium provides a &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/documentation/#connect_transforms"&gt;single message transformation&lt;/a&gt; to provide out-of-the-box support for applications implementing the Outbox pattern. More documentation on the specifics of Debezium’s Outbox Event Router and it’s configuration can be found in the &lt;a target="_blank" rel="nofollow" href="https://debezium.io/documentation/reference/1.1/configuration/outbox-event-router.html"&gt;Debezium documentation&lt;/a&gt;. Since the connector has this capability built-in, we just need to tell Debezium how to map between the fields it expects in the payload and the fields in our actual database table (lines 29-35):&lt;/p&gt; &lt;pre&gt;transforms.outbox.table.field.event.id: event_id transforms.outbox.table.field.event.key: aggregate_id transforms.outbox.table.field.event.timestamp: event_timestamp transforms.outbox.table.field.event.type: event_type transforms.outbox.table.field.event.payload.id: aggregate_id transforms.outbox.route.by.field: aggregate_type transforms.outbox.table.fields.additional.placement: "event_id:envelope:eventId,event_timestamp:envelope:eventTimestamp,aggregate_id:envelope:aggregateId,aggregate_type:envelope:aggregateType"&lt;/pre&gt; &lt;p&gt;We could have named the fields in our table exactly as the Debezium &lt;code&gt;EventRouter&lt;/code&gt; transformation was looking for it, but that would then have tightly-coupled our database schema to Debezium. As a best practice, we want our components to be loosely-coupled and updateable via external configuration.&lt;/p&gt; &lt;p&gt;Now, how do we know this all worked? We can go directly to one of the Kafka broker pods and run the &lt;code&gt;kafka-console-consumer&lt;/code&gt; utility to see the data in the topic.&lt;/p&gt; &lt;h3&gt;Look at resulting events&lt;/h3&gt; &lt;p&gt;Go back to the OpenShift web console and the &lt;strong&gt;Topology&lt;/strong&gt; view. Click the &lt;code&gt;db-events-kafka&lt;/code&gt; resource. When the sidebar appears on the right, click any of the three &lt;code&gt;db-events-kafka&lt;/code&gt; pods that show up (i.e., the list in Figure 20). It doesn’t matter which one.&lt;/p&gt; &lt;div id="attachment_708227" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708227" class="wp-image-708227 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/select-kafka-broker-pod-1024x647.png" alt="Select Kafka broker pod" width="640" height="404" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/select-kafka-broker-pod-1024x647.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/select-kafka-broker-pod-300x189.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/select-kafka-broker-pod-768x485.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/select-kafka-broker-pod.png 1378w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708227" class="wp-caption-text"&gt;Figure 20: Select Kafka broker pod&lt;/p&gt;&lt;/div&gt; &lt;p&gt;From there, click the &lt;b&gt;Terminal&lt;/b&gt; tab to bring you to the terminal. Once at the terminal, run:&lt;/p&gt; &lt;pre&gt;$ bin/kafka-console-consumer.sh --bootstrap-server db-events-kafka-bootstrap:9092 --topic outbox.Album.events --from-beginning &lt;/pre&gt; &lt;p&gt;It will output a bunch of JSON, as shown in Figure 21.&lt;/p&gt; &lt;div id="attachment_708237" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-708237" class="wp-image-708237 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-console-consumer-1024x760.png" alt="Run kafka-console-consumer.sh" width="640" height="475" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-console-consumer-1024x760.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-console-consumer-300x223.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-console-consumer-768x570.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kafka-console-consumer.png 1378w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-708237" class="wp-caption-text"&gt;Figure 21: Run kafka-console-consumer.sh&lt;/p&gt;&lt;/div&gt; &lt;p&gt;You can now examine the raw output. It should look something like this:&lt;/p&gt; &lt;pre&gt;{"schema":{"type":"struct","fields":[{"type":"string","optional":true,"name":"io.debezium.data.Json","version":1,"field":"payload"},{"type":"string","optional":false,"field":"eventType"},{"type":"int64","optional":false,"field":"eventId"},{"type":"int64","optional":false,"name":"io.debezium.time.MicroTimestamp","version":1,"field":"eventTimestamp"},{"type":"string","optional":false,"field":"aggregateId"},{"type":"string","optional":false,"field":"aggregateType"}],"optional":false},"payload":{"payload":"{\"album\": {\"id\": \"9d0a7606-d933-4026-9f33-efa2bde4b9e4\", \"genre\": \"Rock\", \"title\": \"Nevermind\", \"artist\": \"Nirvana\", \"albumId\": null, \"trackCount\": 0, \"releaseYear\": \"1991\"}, \"eventType\": \"ALBUM_CREATED\"}","eventType":"ALBUM_CREATED","eventId":1,"eventTimestamp":1586264029784000,"aggregateId":"9d0a7606-d933-4026-9f33-efa2bde4b9e4","aggregateType":"Album"}}&lt;/pre&gt; &lt;p&gt;The output might not look too legible, but if you pretty-print it (Google &lt;code&gt;json pretty print&lt;/code&gt; in your browser and find a free utility) you’ll see that the payload format looks like this:&lt;/p&gt; &lt;pre&gt;{ "schema": { "type": "struct", "fields": [ { "type": "string", "optional": true, "name": "io.debezium.data.Json", "version": 1, "field": "payload" }, { "type": "string", "optional": false, "field": "eventType" }, { "type": "int64", "optional": false, "field": "eventId" }, { "type": "int64", "optional": false, "name": "io.debezium.time.MicroTimestamp", "version": 1, "field": "eventTimestamp" }, { "type": "string", "optional": false, "field": "aggregateId" }, { "type": "string", "optional": false, "field": "aggregateType" } ], "optional": false }, "payload": { "payload": "{\"album\": {\"id\": \"9d0a7606-d933-4026-9f33-efa2bde4b9e4\", \"genre\": \"Rock\", \"title\": \"Nevermind\", \"artist\": \"Nirvana\", \"albumId\": null, \"trackCount\": 0, \"releaseYear\": \"1991\"}, \"eventType\": \"ALBUM_CREATED\"}", "eventType": "ALBUM_CREATED", "eventId": 1, "eventTimestamp": 1586264029784000, "aggregateId": "9d0a7606-d933-4026-9f33-efa2bde4b9e4", "aggregateType": "Album" } } &lt;/pre&gt; &lt;p&gt;This payload defines its own structure in the &lt;code&gt;schema&lt;/code&gt; element on lines 2-41:&lt;/p&gt; &lt;pre&gt;"schema": { "type": "struct", "fields": [ { "type": "string", "optional": true, "name": "io.debezium.data.Json", "version": 1, "field": "payload" }, { "type": "string", "optional": false, "field": "eventType" }, { "type": "int64", "optional": false, "field": "eventId" }, { "type": "int64", "optional": false, "name": "io.debezium.time.MicroTimestamp", "version": 1, "field": "eventTimestamp" }, { "type": "string", "optional": false, "field": "aggregateId" }, { "type": "string", "optional": false, "field": "aggregateType" } ], "optional": false }&lt;/pre&gt; &lt;p&gt;We could eliminate this section by standing up our own &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-04/html-single/getting_started_with_service_registry/index"&gt;schema registry&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.6/html-single/using_amq_streams_on_openshift/index#service-registry-str"&gt;configuring our cluster to use Avro serialization/deserialization&lt;/a&gt;. The &lt;code&gt;payload&lt;/code&gt; element on lines 42-49 contains metadata about the event, as well as the actual payload of the event:&lt;/p&gt; &lt;pre&gt;"payload": { "payload": "{\"album\": {\"id\": \"9d0a7606-d933-4026-9f33-efa2bde4b9e4\", \"genre\": \"Rock\", \"title\": \"Nevermind\", \"artist\": \"Nirvana\", \"albumId\": null, \"trackCount\": 0, \"releaseYear\": \"1991\"}, \"eventType\": \"ALBUM_CREATED\"}", "eventType": "ALBUM_CREATED", "eventId": 1, "eventTimestamp": 1586264029784000, "aggregateId": "9d0a7606-d933-4026-9f33-efa2bde4b9e4", "aggregateType": "Album" }&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;payload&lt;/code&gt; sub-element within the main payload element (line 43) is itself a JSON string representing the contents of the domain object making up the event:&lt;/p&gt; &lt;pre&gt;"payload": "{\"album\": {\"id\": \"9d0a7606-d933-4026-9f33-efa2bde4b9e4\", \"genre\": \"Rock\", \"title\": \"Nevermind\", \"artist\": \"Nirvana\", \"albumId\": null, \"trackCount\": 0, \"releaseYear\": \"1991\"}, \"eventType\": \"ALBUM_CREATED\"}"&lt;/pre&gt; &lt;p&gt;If you keep this terminal window open and open up a new browser window back to the application itself, you should see new events stream in as you update/delete albums from the application’s user interface.&lt;/p&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;Hopefully, you found this post helpful! If so, please watch for a few other posts in this series once they become available:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding Prometheus metrics &amp;#38; Grafana Dashboard monitoring&lt;/li&gt; &lt;li&gt;Securing Kafka and KafkaConnect with OAuth authentication&lt;/li&gt; &lt;li&gt;Adding access control to Kafka and KafkaConnect with OAuth authorization&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also, if you are like me and want to automate the provisioning of everything, feel free to take a look at an &lt;a target="_blank" rel="nofollow" href="https://github.com/edeandrea/debezium-demo-apb"&gt;Ansible Playbook&lt;/a&gt; that is capable of doing this.&lt;/p&gt; &lt;h2&gt;References&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://debezium.io/documentation/faq/#what_is_change_data_capture"&gt;What is change data capture?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://debezium.io"&gt;Debezium&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.6/html-single/using_amq_streams_on_openshift/index"&gt;Using Red Hat AMQ Streams&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#38;linkname=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F08%2Fchange-data-capture-with-debezium-a-simple-how-to-part-1%2F&amp;#038;title=Change%20data%20capture%20with%20Debezium%3A%20A%20simple%20how-to%2C%20Part%201" data-a2a-url="https://developers.redhat.com/blog/2020/05/08/change-data-capture-with-debezium-a-simple-how-to-part-1/" data-a2a-title="Change data capture with Debezium: A simple how-to, Part 1"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/08/change-data-capture-with-debezium-a-simple-how-to-part-1/"&gt;Change data capture with Debezium: A simple how-to, Part 1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/w4gz00yp2kw" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;One question always comes up as organizations moving towards being cloud-native, twelve-factor, and stateless: How do you get an organization’s data to these new applications? There are many different patterns out there, but one pattern we will look at today is change data capture. This post is a simple how-to on how to build out [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/08/change-data-capture-with-debezium-a-simple-how-to-part-1/"&gt;Change data capture with Debezium: A simple how-to, Part 1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">707777</post-id><dc:creator>Eric Deandrea</dc:creator><dc:date>2020-05-08T07:00:16Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/05/08/change-data-capture-with-debezium-a-simple-how-to-part-1/</feedburner:origLink></entry><entry><title>Keycloak 10.0.1 released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Za-5OqEHD7g/keycloak-1001-released.html" /><category term="feed_group_name_keycloak" scheme="searchisko:content:tags" /><category term="feed_name_keycloak" scheme="searchisko:content:tags" /><category term="Keycloak Release" scheme="searchisko:content:tags" /><author><name>Keycloak</name></author><id>searchisko:content:id:jbossorg_blog-keycloak_10_0_1_released</id><updated>2020-05-08T00:00:00Z</updated><published>2020-05-08T00:00:00Z</published><content type="html">&lt;p&gt;To download the release go to &lt;a href="https://www.keycloak.org//downloads.html"&gt;Keycloak downloads&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;All resolved issues&lt;/h2&gt; &lt;p&gt;The full list of resolved issues are available in &lt;a href="https://issues.jboss.org/issues/?jql=project%20%3D%20keycloak%20and%20fixVersion%20%3D%2010.0.1"&gt;JIRA&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Upgrading&lt;/h2&gt; &lt;p&gt;Before you upgrade remember to backup your database and check the &lt;a href="https://www.keycloak.org//docs/latest/upgrading/index.html"&gt;upgrade guide&lt;/a&gt; for anything that may have changed.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Za-5OqEHD7g" height="1" width="1" alt=""/&gt;</content><summary>To download the release go to Keycloak downloads. All resolved issues The full list of resolved issues are available in JIRA Upgrading Before you upgrade remember to backup your database and check the upgrade guide for anything that may have changed.</summary><dc:creator>Keycloak</dc:creator><dc:date>2020-05-08T00:00:00Z</dc:date><feedburner:origLink>https://www.keycloak.org//2020/05/keycloak-1001-released.html</feedburner:origLink></entry><entry><title>Open Data Hub 0.6 brings component updates and Kubeflow architecture</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/1y54fcYELvw/" /><category term="Big Data" /><category term="Kubernetes" /><category term="Machine Learning" /><category term="Operator" /><category term="AI/ML" /><category term="Kubeflow" /><category term="Open Data Hub" /><category term="openshift" /><author><name>Václav Pavlín</name></author><id>https://developers.redhat.com/blog/?p=717267</id><updated>2020-05-07T07:00:16Z</updated><published>2020-05-07T07:00:16Z</published><content type="html">&lt;p&gt;Open Data Hub (ODH) is a blueprint for building an AI-as-a-service platform on Red Hat&amp;#8217;s &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;-based &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;OpenShift 4.x&lt;/a&gt;. Version 0.6 of Open Data Hub comes with significant changes to the overall architecture as well as component updates and additions. In this article, we explore these changes.&lt;/p&gt; &lt;h2&gt;From Ansible Operator to Kustomize&lt;/h2&gt; &lt;p&gt;If you follow the &lt;a target="_blank" rel="nofollow" href="https://opendatahub.io"&gt;Open Data Hub&lt;/a&gt; project closely, you might be aware that we have been working on a major design change for a few weeks now. Since we started working closer with the &lt;a target="_blank" rel="nofollow" href="https://www.kubeflow.org/"&gt;Kubeflow community&lt;/a&gt; to get &lt;a target="_blank" rel="nofollow" href="https://www.kubeflow.org/docs/openshift/"&gt;Kubeflow running on OpenShift&lt;/a&gt;, we decided to leverage Kubeflow as the Open Data Hub upstream and adopt its deployment tools—namely KFdef manifests and &lt;a target="_blank" rel="nofollow" href="https://kustomize.io/"&gt;Kustomize&lt;/a&gt;—for deployment manifest customization.&lt;/p&gt; &lt;p&gt;&lt;span id="more-717267"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;We still believe &lt;a target="_blank" rel="nofollow" href="https://sdk.operatorframework.io/docs/ansible/quickstart/"&gt;Ansible Operator&lt;/a&gt; provides a great framework for building Operators, but at the same time, we believe we need to be close to our upstream (Kubeflow), which is why it makes perfect sense to align with the project on the deployment and lifecycle management front. For this purpose, we analyzed all of our components and started to rework them from Ansible roles into a Kustomize-compatible structure.&lt;/p&gt; &lt;p&gt;You can find &lt;a target="_blank" rel="nofollow" href="https://github.com/opendatahub-io/odh-manifests"&gt;the new manifests in the &lt;code&gt;odh-manifest repository&lt;/code&gt;&lt;/a&gt;. It closely follows the structure of &lt;a target="_blank" rel="nofollow" href="https://github.com/kubeflow/manifests"&gt;the Kubeflow manifests repository&lt;/a&gt; to make sure that the projects are compatible.&lt;/p&gt; &lt;h2&gt;Updated components&lt;/h2&gt; &lt;p&gt;As part of this rewrite, some of the components were updated as well. We decided to depend on OpenShift 4x in the future starting with version 0.6.0. This decision allowed us to fully take advantage of the &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.4/operators/understanding_olm/olm-understanding-olm.html"&gt;Operator Lifecycle Manager&lt;/a&gt; (OLM) and avoid duplicating the maintenance burdens other teams already undergo by providing their projects in the OLM Catalog.&lt;/p&gt; &lt;p&gt;From a technical perspective, this means that we do not hold deployment manifests for all of the components we deploy and manage, but rather we deploy the Operators through the OLM via a subscription and only instruct the Operator by specific custom resource. The components deployed via OLM are &lt;a target="_blank" rel="nofollow" href="https://strimzi.io/"&gt;Strimzi&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="https://grafana.com/"&gt;Grafana&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s take a look at the other components.&lt;/p&gt; &lt;h3&gt;Spark&lt;/h3&gt; &lt;p&gt;Another component that received an update is the &lt;a target="_blank" rel="nofollow" href="https://github.com/radanalyticsio/spark-operator/"&gt;Radanalytics.io Spark Operator&lt;/a&gt;. We now use &lt;code&gt;SparkCluster&lt;/code&gt; custom resources instead of &lt;code&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/radanalyticsio/spark-operator#config-map-approach"&gt;ConfigMaps&lt;/a&gt;&lt;/code&gt; and we updated our Spark dependencies to Spark 2.4.5.&lt;/p&gt; &lt;h3&gt;Argo&lt;/h3&gt; &lt;p&gt;We originally wanted to use Argo directly through the Kubeflow project, but Kubeflow ships Argo 2.3, which is too old for Open Data Hub users. Since we had to convert the component to Kustomize anyway we decided to also update it to the latest stable version, so ODH now provides &lt;a target="_blank" rel="nofollow" href="https://blog.argoproj.io/argo-workflows-v2-7-6ace8c210798"&gt;Argo 2.7&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;JupyterHub&lt;/h3&gt; &lt;p&gt;JupyterHub changes are coming in the form of updates to the library &lt;a target="_blank" rel="nofollow" href="https://github.com/vpavlin/jupyterhub-singleuser-profiles/"&gt;JupyterHub Singleuser Profiles&lt;/a&gt;, which we use for customizing Jupyter server deployments and for our integrations between JupyterHub and other services like Spark. These changes are mainly related to refactoring some of the configuration structures to be more Kubernetes-native and improving our integration with the Spark Operator.&lt;/p&gt; &lt;h3&gt;Superset&lt;/h3&gt; &lt;p&gt;The list of updated components above does not include Superset, but you do not need to worry—Superset is still part of Open Data Hub. The deployment simply did not change and was converted into Kustomize.&lt;/p&gt; &lt;h3&gt;AI Library&lt;/h3&gt; &lt;p&gt;The last not mentioned component is the AI Library. You can see it in the repository, but we did not officially include it in the 0.6 release since we were not able to get Seldon in, which is a dependency of this library. We will make sure that both AI Library and Seldon are part of the next release.&lt;/p&gt; &lt;h2&gt;Airflow added to Open Data Hub&lt;/h2&gt; &lt;p&gt;We have been hearing requests for &lt;a target="_blank" rel="nofollow" href="https://airflow.apache.org/"&gt;Airflow&lt;/a&gt; in Open Data Hub for a long time. Since we already have a workflow management system (Argo) available in ODH we wanted to make sure that we are not simply duplicating the components. It turns out that many Open Data Hub users are also Airflow users and having Airflow deployed by ODH would add a lot of value for them.&lt;/p&gt; &lt;p&gt;Our team investigated options for adding Airflow among Open Data Hub&amp;#8217;s components and decided to go with the &lt;a target="_blank" rel="nofollow" href="https://github.com/opendatahub-io/airflow-on-k8s-operator"&gt;Airflow Operator&lt;/a&gt;. We also provide an &lt;a target="_blank" rel="nofollow" href="https://github.com/opendatahub-io/odh-manifests/tree/master/airflow/example-celery/base"&gt;example Airflow cluster&lt;/a&gt; definition deploying Airflow with Celery that users can easily enable and try out.&lt;/p&gt; &lt;h2&gt;Installing Open Data Hub 0.6&lt;/h2&gt; &lt;p&gt;As with all of the previous versions of Open Data Hub, we care deeply about the user experience during installation. That’s why we restructured the Operator Catalog entry (Figure 1) to provide two channels—beta and legacy (Figure 2).&lt;/p&gt; &lt;div id="attachment_717287" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-11.png"&gt;&lt;img aria-describedby="caption-attachment-717287" class="wp-image-717287 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-11-1024x524.png" alt="Open Data Hub 0.6 OperatorHub entry" width="640" height="328" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-11-1024x524.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-11-300x154.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-11-768x393.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-717287" class="wp-caption-text"&gt;Figure 1: The Open Data Hub 0.6 OperatorHub entry.&lt;/p&gt;&lt;/div&gt; &lt;div id="attachment_717297" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-39.png"&gt;&lt;img aria-describedby="caption-attachment-717297" class="wp-image-717297 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-39-1024x594.png" alt="Open Data Hub 0.6 channels" width="640" height="371" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-39-1024x594.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-39-300x174.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-39-768x446.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/Screenshot-from-2020-05-04-13-54-39.png 1518w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-717297" class="wp-caption-text"&gt;Figure 2: The channels in Open Data Hub 0.6.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;The beta channel&lt;/h3&gt; &lt;p&gt;The beta channel provides new versions of Open Data Hub starting with ODH 0.6.0. The custom resource, now based on KFDef, has changed significantly along with the implementation, so don’t be surprised by the new example.&lt;/p&gt; &lt;h3&gt;The legacy channel&lt;/h3&gt; &lt;p&gt;The legacy channel still provides an ODH 0.5.x option. At the moment of publishing, it is 0.5.1, but we will do our best to keep bug fixes flowing into version 0.5 as we know that some users need to stay on that version for the time since 0.6 only works on OCP 4. We will not add new features or components to ODH 0.5, though.&lt;/p&gt; &lt;h3&gt;Kubeflow on OpenShift&lt;/h3&gt; &lt;p&gt;One important feature to mention is that since we use the same tooling as Kubeflow, you can use Open Data Hub Operator 0.6 to deploy Kubeflow on OpenShift. The operator only supports KFDef v1, which is newer than what Kubeflow 0.7 contains, so we prepared an updated custom resource for you in &lt;a target="_blank" rel="nofollow" href="https://github.com/opendatahub-io/manifests"&gt;our Kubeflow manifests repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It is not possible to deploy and use Kubeflow and Open Data Hub together at the moment but this feature will be available in upcoming releases.&lt;/p&gt; &lt;h2&gt;Community&lt;/h2&gt; &lt;p&gt;This part does not change a bit—we love to get your feedback. We completely understand that ODH 0.6 comes with significant changes, and it is ever more important for us to know if you hit any issues or have any suggestions.&lt;/p&gt; &lt;p&gt;Since we adopted Kubeflow as our upstream and that community lives on GitHub, we are moving there too. We started our planning for future sprints using &lt;a target="_blank" rel="nofollow" href="https://github.com/orgs/opendatahub-io/projects"&gt;GitHub Projects&lt;/a&gt; and we will iteratively move most of the sources and documentation to GitHub as well, so please bear with us in case the project feels a bit chaotic regarding links and pointers—we are working on it.&lt;/p&gt; &lt;p&gt;In any case, do not hesitate to join our &lt;a target="_blank" rel="nofollow" href="https://gitlab.com/opendatahub/opendatahub-community/-/wikis/Open-Data-Hub-Community-Meeting-Agenda"&gt;community meetings&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://lists.opendatahub.io/admin/lists/"&gt;mailing list&lt;/a&gt;, or simply contact us via &lt;a target="_blank" rel="nofollow" href="https://github.com/opendatahub-io/odh-manifests/issues"&gt;GitHub issues&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#38;linkname=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F07%2Fopen-data-hub-0-6-brings-component-updates-and-kubeflow-architecture%2F&amp;#038;title=Open%20Data%20Hub%200.6%20brings%20component%20updates%20and%20Kubeflow%20architecture" data-a2a-url="https://developers.redhat.com/blog/2020/05/07/open-data-hub-0-6-brings-component-updates-and-kubeflow-architecture/" data-a2a-title="Open Data Hub 0.6 brings component updates and Kubeflow architecture"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/07/open-data-hub-0-6-brings-component-updates-and-kubeflow-architecture/"&gt;Open Data Hub 0.6 brings component updates and Kubeflow architecture&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/1y54fcYELvw" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Open Data Hub (ODH) is a blueprint for building an AI-as-a-service platform on Red Hat&amp;#8217;s Kubernetes-based OpenShift 4.x. Version 0.6 of Open Data Hub comes with significant changes to the overall architecture as well as component updates and additions. In this article, we explore these changes. From Ansible Operator to Kustomize If you follow the [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/07/open-data-hub-0-6-brings-component-updates-and-kubeflow-architecture/"&gt;Open Data Hub 0.6 brings component updates and Kubeflow architecture&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">717267</post-id><dc:creator>Václav Pavlín</dc:creator><dc:date>2020-05-07T07:00:16Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/05/07/open-data-hub-0-6-brings-component-updates-and-kubeflow-architecture/</feedburner:origLink></entry><entry><title>Using Ansible to automate Google Cloud Platform</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/UM8AZASrSmk/" /><category term="CI/CD" /><category term="DevOps" /><category term="Linux" /><category term="Ansible" /><category term="google compute engine" /><category term="rhel 8" /><category term="VM provisioning" /><category term="vpc network" /><author><name>Sreejith Anujan</name></author><id>https://developers.redhat.com/blog/?p=696317</id><updated>2020-05-06T07:00:06Z</updated><published>2020-05-06T07:00:06Z</published><content type="html">&lt;p&gt;In this article, you will learn how to seamlessly automate the provisioning of Google Cloud Platform (GCP) resources using the new &lt;a target="_blank" rel="nofollow" href="https://www.ansible.com/"&gt;Red Hat Ansible&lt;/a&gt; modules and your &lt;a target="_blank" rel="nofollow" href="https://www.ansible.com/products/tower"&gt;Red Hat Ansible Tower&lt;/a&gt; credentials.&lt;/p&gt; &lt;h2&gt;About the new GCP modules&lt;/h2&gt; &lt;p&gt;Starting with &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/roadmap/ROADMAP_2_6.html"&gt;Ansible 2.6&lt;/a&gt;, Red Hat has &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/partners/programs/CCSP"&gt;partnered with Google&lt;/a&gt; to ship a new set of modules for automating Google Cloud Platform resource management. The partnership has resulted in &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/scenario_guides/guide_gce.html"&gt;more than 100 GCP modules&lt;/a&gt; and a consistent naming scheme of &lt;code&gt;gcp_*&lt;/code&gt;. While we still have access to the original modules, developers are recommended to use the newer modules whenever possible.&lt;/p&gt; &lt;p&gt;&lt;span id="more-696317"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;This article quickly gets you started with using the new modules in both &lt;a target="_blank" rel="nofollow" href="https://www.ansible.com/?extIdCarryOver=true&amp;#38;sc_cid=701f2000000RmAOAA0"&gt;Red Hat Ansible&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://www.ansible.com/products/tower?extIdCarryOver=true&amp;#38;sc_cid=701f2000000RmAOAA0"&gt;Red Hat Ansible Tower&lt;/a&gt;. We&amp;#8217;ll start by connecting a Google Cloud Platform service account to your Red Hat Ansible Tower environment. We&amp;#8217;ll then be able to use Ansible Tower to run a simple Ansible playbook that will provision a new virtual machine (VM) disk, virtual private cloud (VPC) network, and IP address for an automated instance of Red Hat Enterprise Linux 8. You&amp;#8217;ll also see how to use Ansible Tower&amp;#8217;s Dynamic Inventory feature to discover the newly created Red Hat Enterprise Linux 8 instance on Google Cloud Platform.&lt;/p&gt; &lt;h2&gt;Connect your GCP service account to Ansible Tower&lt;/h2&gt; &lt;p&gt;This section assumes you already have a &lt;a target="_blank" rel="nofollow" href="https://developers.google.com/identity/protocols/oauth2/service-account#creatinganaccount"&gt;Google Cloud Platform service account&lt;/a&gt;. If you know your account details, the first thing to do is provide the authentication credentials to Ansible Tower. From the Ansible Tower console, select &lt;strong&gt;Credentials &amp;#8211;&amp;#62; New Credential&lt;/strong&gt;. Then select &lt;strong&gt;Google Compute Engine&lt;/strong&gt; as the &lt;strong&gt;Credential Type&lt;/strong&gt;. Figure 1 shows this sequence.&lt;/p&gt; &lt;div id="attachment_696397" style="width: 406px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696397" class="wp-image-696397 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/select_credential_type.png" alt="A screenshot of the dialog to select the credential type." width="396" height="246" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/select_credential_type.png 396w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/select_credential_type-300x186.png 300w" sizes="(max-width: 396px) 100vw, 396px" /&gt;&lt;p id="caption-attachment-696397" class="wp-caption-text"&gt;Figure 1. Select the credential type.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Next, you are asked to provide the details of your Google Cloud Platform service account, as well as naming the project in which you plan to create resources. You can find all of this information in your Google Cloud Platform console.&lt;/p&gt; &lt;p&gt;Figure 2 shows the field where you will enter the email address associated with your Google Cloud Platform service account.&lt;/p&gt; &lt;div id="attachment_696407" style="width: 546px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696407" class="wp-image-696407 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/service_account_email_address.png" alt="A screenshot of the field to enter an email address." width="536" height="70" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/service_account_email_address.png 536w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/service_account_email_address-300x39.png 300w" sizes="(max-width: 536px) 100vw, 536px" /&gt;&lt;p id="caption-attachment-696407" class="wp-caption-text"&gt;Figure 2. Enter the email address associated with your Google Cloud Platform service account&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Next, you&amp;#8217;ll be asked to enter the contents of the Privacy Enhanced Mail (PEM) file associated with your service account email address, as shown in Figure 3.&lt;/p&gt; &lt;div id="attachment_696387" style="width: 479px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696387" class="wp-image-696387 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/rsa_private_key.png" alt="A screenshot of the dialog to enter the private key." width="469" height="120" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/rsa_private_key.png 469w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/rsa_private_key-300x77.png 300w" sizes="(max-width: 469px) 100vw, 469px" /&gt;&lt;p id="caption-attachment-696387" class="wp-caption-text"&gt;Figure 3. Enter the private key for your account.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Finally, you&amp;#8217;ll enter the Google Cloud Platform project ID, as shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_696377" style="width: 548px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696377" class="wp-image-696377 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/project_id.png" alt="A screenshot of the dialog to enter the Google Cloud Platform project ID." width="538" height="82" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/project_id.png 538w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/project_id-300x46.png 300w" sizes="(max-width: 538px) 100vw, 538px" /&gt;&lt;p id="caption-attachment-696377" class="wp-caption-text"&gt;Figure 4. Enter the Google Cloud Platform project ID.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;If you prefer, you can upload a JSON file with your service account details instead of copying and pasting them in. Figure 5 shows the option to select and upload a JSON file.&lt;/p&gt; &lt;div id="attachment_696417" style="width: 550px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696417" class="wp-image-696417 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/service_account_json_file.png" alt="A screenshot of the dialog to select and upload a JSON file." width="540" height="84" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/service_account_json_file.png 540w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/service_account_json_file-300x47.png 300w" sizes="(max-width: 540px) 100vw, 540px" /&gt;&lt;p id="caption-attachment-696417" class="wp-caption-text"&gt;Figure 5. Select and upload a JSON file.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 6 shows the Google Cloud Platform service account, associated email address, and project ID for the environment we&amp;#8217;ve set up so far.&lt;/p&gt; &lt;div id="attachment_696367" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696367" class="wp-image-696367 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_sa_email_project_id-1024x241.png" alt="A screenshot of the completed GCP configuration." width="640" height="151" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_sa_email_project_id-1024x241.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_sa_email_project_id-300x71.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_sa_email_project_id-768x181.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_sa_email_project_id.png 1313w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-696367" class="wp-caption-text"&gt;Figure 6. Configuration for the Google Cloud Platform service account.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;While it is not shown, I have also configured the Google Compute Engine API (&lt;code&gt;compute.googleapis.com&lt;/code&gt;) to access the Google Cloud Platform project, and set the Identity and Access Management (IAM) role to Owner.&lt;/p&gt; &lt;h2&gt;Create a new playbook for the Red Hat Enterprise Linux 8 instance&lt;/h2&gt; &lt;p&gt;After configuring our environment, the next step is to create an Ansible playbook. We&amp;#8217;ll use the Ansible playbook to create a VM disk, a VPC network, an IPv4 address, and finally our new instance of Red Hat Enterprise Linux 8.&lt;/p&gt; &lt;p&gt;I&amp;#8217;m showing the Ansible playbook in Figure 7 as a screenshot to preserve the indentation. You can also &lt;a target="_blank" rel="nofollow" href="http://people.redhat.com/sanujan/gcp_resources.yml"&gt;download the playbook&lt;/a&gt; and use it as an example.&lt;/p&gt; &lt;div id="attachment_701197" style="width: 592px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-701197" class="wp-image-701197 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/Ansible_GCP_Playbook.png" alt="A screenshot of the Ansible playbook." width="582" height="886" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/Ansible_GCP_Playbook.png 582w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/Ansible_GCP_Playbook-197x300.png 197w" sizes="(max-width: 582px) 100vw, 582px" /&gt;&lt;p id="caption-attachment-701197" class="wp-caption-text"&gt;Figure 7. The Ansible playbook to create a Red Hat Enterprise Linux 8 instance.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Let&amp;#8217;s quickly look at each task in the playbook:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Task 1: Create the VM disk&lt;/strong&gt;: We can use a source image to create a VM disk and make the disk bootable. In this case, we&amp;#8217;ll use the certified Red Hat Enterprise Linux 8 image available from Google Cloud Platform. The &lt;code&gt;gcp_compute_disk&lt;/code&gt; module uses the &lt;code&gt;rhel-8-v20190905&lt;/code&gt; image to add a persistent disk for our Red Hat Enterprise Linux 8 instance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task 2: Create the VPC network&lt;/strong&gt;: Next, the &lt;code&gt;gcp_compute_network&lt;/code&gt; module creates a VPC network. The Red Hat Enterprise Linux instance will have an interface associated with that network.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task 3: Create the IPv4 address&lt;/strong&gt;: The &lt;code&gt;gcp_compute_address&lt;/code&gt; module allocates an external IPv4 address to be associated with the Red Hat Enterprise Linux instance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task 4: Create the Red Hat Enterprise Linux instance&lt;/strong&gt;: The &lt;code&gt;gcp_compute_instance&lt;/code&gt; module uses the resources from the previous tasks to create an instance of Red Hat Enterprise Linux 8.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task 5: Check the IPv4 address&lt;/strong&gt;: The debug module shows the IPv4 address associated with the Red Hat Enterprise Linux 8 instance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note that we will execute the playbook within Ansible Tower. We&amp;#8217;ll use the &lt;code&gt;auth_kind&lt;/code&gt; attribute to reference the Google Cloud Platform credentials required for each task. We&amp;#8217;ll then use the &lt;code&gt;gcp_cred_kind&lt;/code&gt; variable to map the tasks to our Google Cloud Platform &lt;code&gt;serviceaccount&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;During execution, Ansible Tower will use the &lt;code&gt;GCP_AUTH_KIND&lt;/code&gt; environment variable to reference and pass the Google Cloud Platform service account&amp;#8217;s email, project, and private-key contents to the playbook.&lt;/p&gt; &lt;h3&gt;Execute the playbook&lt;/h3&gt; &lt;p&gt;Next, we want to create a new job template to execute the playbook in Ansible Tower. Before we can do that, we need to create an inventory and a project referencing the playbook. For this demo, the project is &lt;strong&gt;GCP&lt;/strong&gt; and the inventory is &lt;strong&gt;GCP_Provision_Resources&lt;/strong&gt;, as shown in Figure 8.&lt;/p&gt; &lt;div id="attachment_696357" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696357" class="wp-image-696357 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_job_template-1024x264.png" alt="A screenshot of the configuration for the playbook inventory and project." width="640" height="165" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_job_template-1024x264.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_job_template-300x77.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_job_template-768x198.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_job_template.png 1239w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-696357" class="wp-caption-text"&gt;Figure 8. Create an inventory and a project referencing the playbook you want to run.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Run the job template&lt;/h3&gt; &lt;p&gt;Now, we can run the job template. Check the Google Cloud Platform console to see the new resources being created as each individual task is executed. From the job template output shown in Figure 9, you can see that the new resources were created.&lt;/p&gt; &lt;div id="attachment_696327" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696327" class="wp-image-696327 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/executing_job_template-1024x567.png" alt="A screenshot of job template output." width="640" height="354" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/executing_job_template-1024x567.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/executing_job_template-300x166.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/executing_job_template-768x425.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/executing_job_template.png 1053w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-696327" class="wp-caption-text"&gt;Figure 9. Job template output shows the new resources have been created.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;You can also use the Google Cloud Platform console to verify the resources. In Figure 10, you can see the VM disk, &lt;code&gt;disk-instance&lt;/code&gt;, was created with a size of 50 GB.&lt;/p&gt; &lt;div id="attachment_696427" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696427" class="wp-image-696427 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_disk-1024x239.png" alt="A screenshot of the VM disk listed in the Google Compute Engine console." width="640" height="149" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_disk-1024x239.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_disk-300x70.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_disk-768x179.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_disk.png 1177w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-696427" class="wp-caption-text"&gt;Figure 10. The VM disk has been created.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 11 shows that the VPC network, &lt;code&gt;network-instance&lt;/code&gt;, was created on subnet 10.240.0.0/16.&lt;/p&gt; &lt;div id="attachment_696457" style="width: 517px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696457" class="wp-image-696457 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_network.png" alt="A screenshot showing the VPC network hosted on Google Cloud Platform." width="507" height="387" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_network.png 507w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_network-300x229.png 300w" sizes="(max-width: 507px) 100vw, 507px" /&gt;&lt;p id="caption-attachment-696457" class="wp-caption-text"&gt;Figure 11. The VPC network has been created.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 12 shows that the external public IPv4 address of 35.192.86.87 is associated with the newly created Red Hat Enterprise Linux 8 instance.&lt;/p&gt; &lt;div id="attachment_696437" style="width: 649px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance.png"&gt;&lt;img aria-describedby="caption-attachment-696437" class="wp-image-696437" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance.png" alt="A screenshot of the external public IPv4 address." width="639" height="143" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance.png 991w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance-300x67.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance-768x171.png 768w" sizes="(max-width: 639px) 100vw, 639px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-696437" class="wp-caption-text"&gt;Figure 12. The external public IPv4 address is listed.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Finally, in Figure 13 we see that the new Red Hat Enterprise Linux 8 instance was created. The new instance, &lt;code&gt;rhel8&lt;/code&gt;, has an internal IP address of 10.240.0.2 and a public external IP address of 35.192.86.87.&lt;/p&gt; &lt;div id="attachment_696437" style="width: 651px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance.png"&gt;&lt;img aria-describedby="caption-attachment-696437" class="wp-image-696437" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance.png" alt="A screenshot showing the new instance on Google Compute Engine." width="641" height="143" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance.png 991w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance-300x67.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/verify_gcp_instance-768x171.png 768w" sizes="(max-width: 641px) 100vw, 641px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-696437" class="wp-caption-text"&gt;Figure 13. The new Red Hat Enterprise Linux 8 instance has been created.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Create a dynamic inventory for GCP instances&lt;/h2&gt; &lt;p&gt;Ansible Tower lets you periodically sync with the Google Cloud API to find realtime instance counts and details for resources hosted on Google Cloud Platform.&lt;/p&gt; &lt;p&gt;To create a new inventory, choose &lt;strong&gt;Google Compute Engine&lt;/strong&gt; as the source, then select the Google Cloud Platform credential you created at the beginning of this article. Optionally, you can limit the sync to specific GCP regions, as shown in Figure 14.&lt;/p&gt; &lt;div id="attachment_696337" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696337" class="wp-image-696337 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory-1024x258.png" alt="A screenshot of the new inventory." width="640" height="161" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory-1024x258.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory-300x76.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory-768x194.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory.png 1598w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-696337" class="wp-caption-text"&gt;Figure 14. Create a new inventory.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Next, use the &lt;strong&gt;Inventory Sync&lt;/strong&gt; button to synchronize the newly created instance &lt;code&gt;rhel8&lt;/code&gt; in the &lt;strong&gt;US Central (A)&lt;/strong&gt; region.&lt;/p&gt; &lt;p&gt;Once synced, check the new hosts in the inventory. As you can see in Figure 15, the new instance is listed.&lt;/p&gt; &lt;div id="attachment_696347" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-696347" class="wp-image-696347 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory_sync-1024x218.png" alt="A screenshot showing the new instance is listed." width="640" height="136" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory_sync-1024x218.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory_sync-300x64.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory_sync-768x164.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/gcp_dynamic_inventory_sync.png 1305w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-696347" class="wp-caption-text"&gt;Figure 15. The new instance is listed.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, you&amp;#8217;ve seen how to use the new &lt;code&gt;gcp_*&lt;/code&gt; modules to automate the provisioning of resources on Google Cloud Platform. I also quickly showed you how to create a new dynamic inventory to periodically sync with the Google Cloud API and find realtime instance counts and details for resources hosted on Google Cloud Platform. Using the Dynamic Inventory feature is a recommended practice for large, fast-changing cloud environments where systems are frequently deployed, tested, and then removed.&lt;/p&gt; &lt;p&gt;Here is a video demonstration of the activities covered in this article.&lt;/p&gt; &lt;p&gt;&lt;iframe class='youtube-player' type='text/html' width='640' height='360' src='https://www.youtube.com/embed/v6dwOPkA-bA?version=3&amp;#038;rel=1&amp;#038;fs=1&amp;#038;autohide=2&amp;#038;showsearch=0&amp;#038;showinfo=1&amp;#038;iv_load_policy=1&amp;#038;wmode=transparent' allowfullscreen='true' style='border:0;'&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;h2&gt;Learn more&lt;/h2&gt; &lt;p&gt;See the following resources to learn more about topics in this article:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/index.html"&gt;About Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible-tower/"&gt;About Ansible Tower&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/modules/list_of_cloud_modules.html#google"&gt;The new Google Cloud Platform modules for Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible-tower/latest/html/quickinstall/prepare.html"&gt;Installation instructions for Ansible Tower&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://cloud.google.com/deployment-manager/docs/step-by-step-guide/installation-and-setup"&gt;Installation and setup instructions for Google Cloud&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#38;linkname=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F06%2Fusing-ansible-to-automate-google-cloud-platform%2F&amp;#038;title=Using%20Ansible%20to%20automate%20Google%20Cloud%20Platform" data-a2a-url="https://developers.redhat.com/blog/2020/05/06/using-ansible-to-automate-google-cloud-platform/" data-a2a-title="Using Ansible to automate Google Cloud Platform"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/06/using-ansible-to-automate-google-cloud-platform/"&gt;Using Ansible to automate Google Cloud Platform&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/UM8AZASrSmk" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In this article, you will learn how to seamlessly automate the provisioning of Google Cloud Platform (GCP) resources using the new Red Hat Ansible modules and your Red Hat Ansible Tower credentials. About the new GCP modules Starting with Ansible 2.6, Red Hat has partnered with Google to ship a new set of modules for [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/06/using-ansible-to-automate-google-cloud-platform/"&gt;Using Ansible to automate Google Cloud Platform&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">696317</post-id><dc:creator>Sreejith Anujan</dc:creator><dc:date>2020-05-06T07:00:06Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/05/06/using-ansible-to-automate-google-cloud-platform/</feedburner:origLink></entry><entry><title>Working with big spatial data workflows (or, what would John Snow do?)</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Idzz0tQQ5zY/" /><category term="Big Data" /><category term="Java" /><category term="JavaScript" /><category term="Microservices" /><category term="apache camel" /><category term="cluster map" /><category term="low code" /><category term="Red Hat Fuse" /><category term="syndesis" /><author><name>Maria Arias de Reyna Dominguez</name></author><id>https://developers.redhat.com/blog/?p=705317</id><updated>2020-05-05T07:00:03Z</updated><published>2020-05-05T07:00:03Z</published><content type="html">&lt;p&gt;With the rise of social networks and people having more free time due to isolation, it has become popular to see lots of maps and graphs. These are made using big spatial data to explain how COVID-19 is expanding, why it is faster in some countries, and how we can stop it.&lt;/p&gt; &lt;p&gt;Some of these maps and graphs are made by inexperienced amateurs that have access to huge amounts of raw and processed big spatial data. But most of them are not sure how to handle that data. A few unaware amateurs mix different sources without caring about homogenizing the data first. Some others mix old data with new. And finally, most forget to add relevant variables because this is too much data to handle manually.&lt;/p&gt; &lt;p&gt;How would a professional handle all of this?&lt;/p&gt; &lt;h2&gt;The cholera outbreak&lt;/h2&gt; &lt;p&gt;In situations where we have to handle big spatial data, I can&amp;#8217;t help but wonder: What would John Snow do? I&amp;#8217;m not talking about that &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/Jon_Snow_(character)"&gt;warrior in the cold north fighting zombies&lt;/a&gt;. I&amp;#8217;m talking about the original &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/John_Snow"&gt;John Snow&lt;/a&gt;, an English doctor from the XIX century that used spatial data to study a cholera outbreak.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s go back to 1854, London, where a &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/1854_Broad_Street_cholera_outbreak"&gt;cholera outbreak&lt;/a&gt; was taking heavy casualties. Most doctors at the time, unaware of germs, thought it was caused by &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/Miasma_theory"&gt;miasma&lt;/a&gt;, a kind of bad air that polluted people, making them ill.&lt;/p&gt; &lt;h3&gt;John Snow data analysis&lt;/h3&gt; &lt;p&gt;But John was not convinced by that theory. He had a hypothesis on what the real cause could be, suspecting water-related issues. He collected data on where the people infected lived and where they got their water from and ran some spatial data analysis to prove those ideas. Figure 1 shows one of his original maps.&lt;/p&gt; &lt;div id="attachment_705337" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-705337" class="wp-image-705337 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/1280px-Snow-cholera-map-1-1024x954.jpg" alt="Original map by John Snow showing the clusters of cholera cases (indicated by stacked rectangles) in the London epidemic of 1854" width="640" height="596" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/1280px-Snow-cholera-map-1-1024x954.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/1280px-Snow-cholera-map-1-300x280.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/1280px-Snow-cholera-map-1-768x716.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/1280px-Snow-cholera-map-1.jpg 1280w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-705337" class="wp-caption-text"&gt;Figure 1: Original map by John Snow showing the clusters of cholera cases in the London epidemic of 1854.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;With that accurate data, he was able to generate a cluster map showing the spread of the disease. This work helped him prove his theories on cholera&amp;#8217;s water origin. He had only a few sources of data, but they were all homogeneous. Plus, he was able to collect data directly in the field, making sure it was accurate and met his needs.&lt;/p&gt; &lt;p&gt;It&amp;#8217;s important to notice that because he used the right data, he arrived at the right conclusions. He studied the outliers, like those people drinking water from a different source than what should have been the closest to their homes. Thus he was able to conflate the data with the proper sources, curating it. Homogenizing and conflating the sources of data is a relevant step to arrive at the right conclusions.&lt;/p&gt; &lt;p&gt;John Snow had to manually conflate and analyze all of the data and it was a good choice. The amount of data he handled was fit for working with pen and paper. But in our case, when we try to conflate all the sources available worldwide, what we are really facing is big spatial data, which is impossible to handle manually.&lt;/p&gt; &lt;h2&gt;Big spatial data&lt;/h2&gt; &lt;p&gt;Not only do we have the specific related data, but we also have data about different isolation or social distancing norms, health care, personal savings, access to clean water, diet, population density, population age, and previous health care issues. The amount of related data available is huge.&lt;/p&gt; &lt;p&gt;Remember, if your data fits into a hard disk, that&amp;#8217;s hardly big data. We are talking here about the amount of data that calls for unending data storage on server farms. No analyst can update, conflate, and analyze all that data manually. We need tools, good tools, to be able to deliver reliable results.&lt;/p&gt; &lt;p&gt;Consider that different data collectors update their data in almost real-time but at different rates, and each country has its own statistics and its own way to measure each variable. So, you need to transform and homogenize before conflating those sources.&lt;/p&gt; &lt;p&gt;How can we keep up-to-date without going crazy? Before you can finish even half of the workflow shown in Figure 2, there&amp;#8217;s freshly new data waiting for you.&lt;/p&gt; &lt;div id="attachment_705397" style="width: 769px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-705397" class="wp-image-705397 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/workflow.png" alt="Workflow for big spatial data: Update → Homogenize → Conflate → Analyze" width="759" height="127" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/workflow.png 759w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/workflow-300x50.png 300w" sizes="(max-width: 759px) 100vw, 759px" /&gt;&lt;p id="caption-attachment-705397" class="wp-caption-text"&gt;Figure 2: We need to run this workflow continuously to always use the newest big spatial data available.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;What would John Snow do? Well, I&amp;#8217;m quite sure he would like all of us to use the proper tools for the work. That&amp;#8217;s why it&amp;#8217;s called &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/Location_intelligence"&gt;Location &lt;em&gt;Intelligence&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Middleware to the rescue&lt;/h2&gt; &lt;p&gt;Regarding those four steps, there are three that can be automated: update, homogenize, and conflate. All of those are tedious and repetitive tasks that make developers quickly jump into scripting rough code. And we know what happens when we write quickly supporting code: We tend to make the same mistakes that others already fixed.&lt;/p&gt; &lt;p&gt;Well, here we are lucky. We have several free and open source software libraries and frameworks that can help us through these tasks. These tools can be found in the &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/technologies/jboss-middleware/fuse"&gt;Red Hat Fuse Integration Platform&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Apache Camel&lt;/h3&gt; &lt;p&gt;Our first option should always be using &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/"&gt;Apache Camel&lt;/a&gt; to help us create complex data workflows. With this framework, we can periodically extract the latest data from different sources, transform, and conflate automatically. We can even use &lt;a href="https://developers.redhat.com/blog/2019/08/27/devnation-live-kubernetes-enterprise-integration-patterns-with-camel-k/"&gt;Camel K&lt;/a&gt; and leave it running on some Kubernetes container while we focus on the non-automatable steps of our work.&lt;/p&gt; &lt;p&gt;Defining workflows in Camel is easy. You can use &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/manual/latest/languages.html"&gt;different common languages&lt;/a&gt; such as &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/manual/latest/java-dsl.html"&gt;Java&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/camel-k/latest/languages/javascript.html"&gt;Javascript&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/camel-k/latest/languages/groovy.html"&gt;Groovy,&lt;/a&gt; or a specific &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/manual/latest/dsl.html"&gt;domain-specific language (DSL)&lt;/a&gt;. With &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/components/latest/index.html"&gt;Camel&amp;#8217;s hundreds of components&lt;/a&gt;, you can feed your workflow with almost any source of data, process the data, and output the processed data in the format your analysis requires.&lt;/p&gt; &lt;h3&gt;Syndesis&lt;/h3&gt; &lt;p&gt;For those data analysts that are less tech-savvy and feel that writing Camel scripts is too complex, we also have &lt;a target="_blank" rel="nofollow" href="https://syndesis.io/"&gt;Syndesis&lt;/a&gt;. With Syndesis you can &lt;a href="https://developers.redhat.com/blog/2020/03/25/low-code-microservices-orchestration-with-syndesis/"&gt;define data workflows&lt;/a&gt; in a more visual way, as you can see in Figure 3.&lt;/p&gt; &lt;div id="attachment_705627" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-705627" class="wp-image-705627 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis1-1024x507.png" alt="Syndesis Integrations Overview" width="640" height="317" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis1-1024x507.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis1-300x149.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis1-768x380.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis1.png 1030w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-705627" class="wp-caption-text"&gt;Figure 3: We can define several processes on Syndesis, each running based on a different trigger.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;This means you can update that big spatial data without having to write a single line of code. Or, maybe you just want to speed up the workflow creation process to jump directly into the analysis.&lt;/p&gt; &lt;p&gt;We can either create one single workflow or break it down to several workflows, as shown in Figure 4. For example, the first process could be triggered by a timer to download different data sources and send that raw data to a Kafka broker. Then, a second process could listen to that broker, transform and homogenize the data previously downloaded, and store it on some common data storage. Finally, a third process can take several sources of data from that common storage with homogenized data, conflate those sources, and prepare the data for further analysis or exposition.&lt;/p&gt; &lt;div id="attachment_705637" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-705637" class="wp-image-705637 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis2-1024x777.png" alt="Syndesis Integration Creation View for a process to update big spatial data" width="640" height="486" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis2-1024x777.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis2-300x228.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis2-768x583.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/Syndesis2.png 1031w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-705637" class="wp-caption-text"&gt;Figure 4: We can easily add steps to the workflow using that plus button.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Note that each step can filter, transform, and use data from different sources, allowing us to create complex workflows in a simple and visual way. We can run the data through different APIs, XSLT transformations, data mapping, and filters to make sure we end up with data ready for analysis.&lt;/p&gt; &lt;h2&gt;The final touch&lt;/h2&gt; &lt;p&gt;Now that we have our data updated, homogenized, transformed, and conflated, we can start the analysis. As both Camel and Syndesis can provide the output in different formats, we can connect it to any software we need to do this analysis. From databases like &lt;a target="_blank" rel="nofollow" href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; to XML-based data formats like &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/Keyhole_Markup_Language"&gt;KML&lt;/a&gt;, we could feed our analysis tools the way we need.&lt;/p&gt; &lt;p&gt;For example, we can use &lt;a target="_blank" rel="nofollow" href="https://qgis.org"&gt;QGIS&lt;/a&gt;, which is an advanced desktop application for data analysis. You can add all those already transformed and conflated big spatial data sources to QGIS to create beautiful graphs and maps as outputs. After that, you can publish your maps with &lt;a target="_blank" rel="nofollow" href="https://openlayers.org/"&gt;OpenLayers&lt;/a&gt; or &lt;a target="_blank" rel="nofollow" href="https://leafletjs.com/"&gt;Leaflet&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Make John Snow proud! And do it using free and open source software.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#38;linkname=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F05%2Fworking-with-big-spatial-data-workflows-or-what-would-john-snow-do%2F&amp;#038;title=Working%20with%20big%20spatial%20data%20workflows%20%28or%2C%20what%20would%20John%20Snow%20do%3F%29" data-a2a-url="https://developers.redhat.com/blog/2020/05/05/working-with-big-spatial-data-workflows-or-what-would-john-snow-do/" data-a2a-title="Working with big spatial data workflows (or, what would John Snow do?)"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/05/working-with-big-spatial-data-workflows-or-what-would-john-snow-do/"&gt;Working with big spatial data workflows (or, what would John Snow do?)&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Idzz0tQQ5zY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;With the rise of social networks and people having more free time due to isolation, it has become popular to see lots of maps and graphs. These are made using big spatial data to explain how COVID-19 is expanding, why it is faster in some countries, and how we can stop it. Some of these [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/05/working-with-big-spatial-data-workflows-or-what-would-john-snow-do/"&gt;Working with big spatial data workflows (or, what would John Snow do?)&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">705317</post-id><dc:creator>Maria Arias de Reyna Dominguez</dc:creator><dc:date>2020-05-05T07:00:03Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/05/05/working-with-big-spatial-data-workflows-or-what-would-john-snow-do/</feedburner:origLink></entry><entry><title>Monitor business metrics with Red Hat Process Automation Manager, Elasticsearch, and Kibana</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/BeZggEwPQzQ/" /><category term="DevOps" /><category term="Event-Driven" /><category term="Java" /><category term="Microservices" /><category term="data visualization" /><category term="elastic search" /><category term="Kibana" /><category term="KPI metrics" /><category term="openshift" /><author><name>snandaku</name></author><id>https://developers.redhat.com/blog/?p=706837</id><updated>2020-05-04T07:00:34Z</updated><published>2020-05-04T07:00:34Z</published><content type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhpam/overview"&gt;Red Hat Process Automation Manager&lt;/a&gt; is a platform for developing containerized microservices and applications that automate business decisions and processes. Combining process- and task-level SLA metrics plus case-related breakdowns can be beneficial for identifying trends and reorganizing the workforce as necessary. So, a critical piece of a business process system is having real-time insights into what is happening, and both monitoring KPI metrics and responding to problem trends is an integral part of operations.&lt;/p&gt; &lt;p&gt;Integration with Elasticsearch improves our search capabilities and provides a unified reporting environment for the business. &lt;a target="_blank" rel="nofollow" href="http://mswiderski.blogspot.com/2017/08/elasticsearch-empowers-jbpm.html"&gt;Maciej Swiderski blogged about&lt;/a&gt; how we can potentially use Elasticsearch to capture KPI metrics and provide for full-text search capabilities. This article extends the idea and walks through how to enable integration with Elasticsearch on a &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; environment, and how to represent the KPIs in a graphical business-friendly dashboard using Kibana.&lt;/p&gt; &lt;h2&gt;&lt;b&gt;Preparing the demo environment&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;Let’s install the necessary components for this demonstration on Red Hat OpenShift, which enables efficient container orchestration and allows rapid container provisioning, deployment, scaling, and management.&lt;/p&gt; &lt;h3&gt;Setting up Elastic and Kibana&lt;/h3&gt; &lt;p&gt;Let&amp;#8217;s make use of the Elastic cluster Operator to set up Elastic/Kibana on OpenShift:&lt;/p&gt; &lt;pre&gt;$ oc apply -f &lt;a target="_blank" rel="nofollow" href="https://download.elastic.co/downloads/eck/1.0.1/all-in-one.yaml"&gt;https://download.elastic.co/downloads/eck/1.0.1/all-in-one.yaml&lt;/a&gt; $ oc new-project elastic&lt;/pre&gt; &lt;p&gt;Now, we can deploy an Elastic instance:&lt;/p&gt; &lt;pre&gt;$ cat &amp;#60;&amp;#60;EOF | oc apply -n elastic -f - # This sample sets up an Elasticsearch cluster with an OpenShift route apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata:  name: elasticsearch-sample spec:  version: 7.6.2  nodeSets:  - name: default    count: 1    config:      node.master: true      node.data: true      node.ingest: true      node.store.allow_mmap: false --- apiVersion: route.openshift.io/v1 kind: Route metadata:  name: elasticsearch-sample spec:  #host: elasticsearch.example.com # override if you don't want to use the host that is automatically generated by OpenShift (&amp;#60;route-name&amp;#62;[-&amp;#60;namespace&amp;#62;].&amp;#60;suffix&amp;#62;)  tls:    termination: passthrough # Elasticsearch is the TLS endpoint    insecureEdgeTerminationPolicy: Redirect  to:    kind: Service    name: elasticsearch-sample-es-http EOF&lt;/pre&gt; &lt;p&gt;Next, let us deploy a Kibana instance:&lt;/p&gt; &lt;pre&gt;$ cat &amp;#60;&amp;#60;EOF | oc apply -n elastic -f - apiVersion: kibana.k8s.elastic.co/v1 kind: Kibana metadata:  name: kibana-sample spec:  version: 7.6.2  count: 1  elasticsearchRef:    name: "elasticsearch-sample"  podTemplate:    spec:      containers:      - name: kibana        resources:          limits:            memory: 1Gi            cpu: 1 --- apiVersion: v1 kind: Route metadata:  name: kibana-sample spec:  #host: kibana.example.com # override if you don't want to use the host that is automatically generated by OpenShift (&amp;#60;route-name&amp;#62;[-&amp;#60;namespace&amp;#62;].&amp;#60;suffix&amp;#62;)  tls:    termination: passthrough # Kibana is the TLS endpoint    insecureEdgeTerminationPolicy: Redirect  to:    kind: Service    name: kibana-sample-kb-http EOF&lt;/pre&gt; &lt;p&gt;We can now access the Kibana dashboard from the route exposed:&lt;/p&gt; &lt;pre&gt;$ oc get route -n elastic &lt;/pre&gt; &lt;p&gt;The credentials for logging into Kibana can be found under &lt;strong&gt;Secrets&lt;/strong&gt; for the Elastic project, as shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_706927" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/elastic_secret.png"&gt;&lt;img aria-describedby="caption-attachment-706927" class="wp-image-706927 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/elastic_secret-1024x486.png" alt="The login credentials for Kibana are available in Secrets" width="640" height="304" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/elastic_secret-1024x486.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/elastic_secret-300x143.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/elastic_secret-768x365.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/elastic_secret.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-706927" class="wp-caption-text"&gt;Figure 1: The login credentials for Kibana are available in Secrets.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;&lt;b&gt;Setting up the Elasticsearch event emitter &lt;/b&gt;&lt;/p&gt; &lt;p&gt;The event emitter integration code is a single Java class that implements the &lt;code&gt;EventEmitter&lt;/code&gt; interface. A basic implementation of the emitter &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/tree/master/jbpm-event-emitters/jbpm-event-emitters-elasticsearch"&gt;can be found here&lt;/a&gt;. By default process, task, and case metrics are pushed on to the corresponding indexes on Elastic:&lt;/p&gt; &lt;pre&gt;if (view instanceof ProcessInstanceView) { index = "processes"; type = "process"; id = ((ProcessInstanceView) view).getCompositeId(); } else if (view instanceof TaskInstanceView) { index = "tasks"; type = "task"; id = ((TaskInstanceView) view).getCompositeId(); } else if (view instanceof CaseInstanceView) { index = "cases"; type = "case"; id = ((CaseInstanceView) view).getCompositeId(); } content.append("{ \"index\" : { \"_index\" : \"" + index + "\", \"_type\" : \"" + type + "\", \"_id\" : \"" + id + "\" } }\n"); content.append(json); &lt;/pre&gt; &lt;p&gt;We can now extend this basic implementation and allow for HTTPS authentication because the Elastic instance on OpenShift is exposed over HTTPS:&lt;/p&gt; &lt;pre&gt;protected CloseableHttpClient buildClient() throws Exception{   HttpClientBuilder builder = HttpClients.custom();   if (elasticSearchUser != null &amp;#38;&amp;#38; elasticSearchPassword != null) {       SSLContextBuilder builder1 = new SSLContextBuilder();       builder1.loadTrustMaterial(null, new TrustStrategy() {           @Override           public boolean isTrusted(X509Certificate[] chain, String authType) throws CertificateException {               return true;           }       });       CredentialsProvider provider = new BasicCredentialsProvider();       UsernamePasswordCredentials credentials = new UsernamePasswordCredentials(elasticSearchUser, elasticSearchPassword);       provider.setCredentials(AuthScope.ANY, credentials);       SSLConnectionSocketFactory sslConnectionSocketFactory = new SSLConnectionSocketFactory(builder1.build(), new NoopHostnameVerifier());       builder.setDefaultCredentialsProvider(provider);       builder.setSSLSocketFactory(sslConnectionSocketFactory).build();   }   return builder.build(); }&lt;/pre&gt; &lt;p&gt;Please note that for the purpose of this demonstration, we bypassed the certificate check and used the basic authentication mechanism. For a production use case, it is necessary to authenticate with a valid certificate.&lt;/p&gt; &lt;p&gt;We can now build a custom Docker image with the event emitter JAR on the KIE server war’s classpath:&lt;/p&gt; &lt;pre&gt;FROM docker-registry.default.svc:5000/openshift/rhpam-kieserver-rhel8:7.5.0 COPY contrib/jbpm-event-emitters-elasticsearch-7.36.0-SNAPSHOT.jar /opt/eap/standalone/deployments/ROOT.war/WEB-INF/lib/jbpm-event-emitters-elasticsearch-7.36.0-SNAPSHOT.jar USER root RUN chownjboss:root /opt/eap/standalone/deployments/ROOT.war/WEB-INF/lib/jbpm-event-emitters-elasticsearch-7.36.0-SNAPSHOT.jar &amp;#38;&amp;#38; \ chmod 664 /opt/eap/standalone/deployments/ROOT.war/WEB-INF/lib/jbpm-event-emitters-elasticsearch-7.36.0-SNAPSHOT.jar USER jboss&lt;/pre&gt; &lt;p&gt;We also need to pass the corresponding metadata for the Elastic cluster using the &lt;code&gt;JAVA_OPTS&lt;/code&gt; Kie Server property:&lt;/p&gt; &lt;pre&gt;$ oc set env dc/{{ pam_app_name }}-kieserver JAVA_OPTS_APPEND=&amp;#62;\"-Dorg.jbpm.event.emitters.elasticsearch.url=https://{{routeelastic.stdout}} -Dorg.jbpm.event.emitters.elasticsearch.user=elastic -Dorg.jbpm.event.emitters.elasticsearch.password={{elasticpwd.stdout}}\" -n {{ OCP_PROJECT }}&lt;/pre&gt; &lt;h2&gt;&lt;b&gt;Business activity monitoring&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;Now that we are done with the setup, let us quickly look at to visualize the process, task, and case metrics using Kibana. Let us begin by logging into Business Central and pulling a project from the &lt;strong&gt;Try Samples&lt;/strong&gt; section, as shown in Figures 2 and 3.&lt;/p&gt; &lt;div id="attachment_706947" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_proj_samples.png"&gt;&lt;img aria-describedby="caption-attachment-706947" class="wp-image-706947 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_proj_samples-1024x446.png" alt="Business Central -&amp;#62; Space -&amp;#62; MySpace -&amp;#62; Projects -&amp;#62; dropdown list box -&amp;#62; Try Sample" width="640" height="279" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_proj_samples-1024x446.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_proj_samples-300x131.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_proj_samples-768x335.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_proj_samples.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-706947" class="wp-caption-text"&gt;Figure 2: Pulling up the Try Samples section.&lt;/p&gt;&lt;/div&gt; &lt;div id="attachment_706957" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/samples.png"&gt;&lt;img aria-describedby="caption-attachment-706957" class="wp-image-706957 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/samples-1024x476.png" alt="The Try Samples project as it appears in MySpace" width="640" height="298" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/samples-1024x476.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/samples-300x139.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/samples-768x357.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/samples.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-706957" class="wp-caption-text"&gt;Figure 3: The Try Samples section contains a variety of options.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;For this example, we will build and deploy the &lt;strong&gt;Mortgage_Process&lt;/strong&gt; project. We will use this project for metrics visualization. Let us now kickstart a process using the process start form:&lt;/p&gt; &lt;div id="attachment_706967" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_form.png"&gt;&lt;img aria-describedby="caption-attachment-706967" class="wp-image-706967 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_form-1024x516.png" alt="" width="640" height="323" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_form-1024x516.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_form-300x151.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_form-768x387.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/new_form.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-706967" class="wp-caption-text"&gt;Figure 4: Enter the details into the process start form.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The process is created, as shown in Figure 5.&lt;/p&gt; &lt;div id="attachment_706977" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_creation.png"&gt;&lt;img aria-describedby="caption-attachment-706977" class="wp-image-706977 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_creation-1024x476.png" alt="MortgageApprovalProcess shown in the Process Instance perspective" width="640" height="298" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_creation-1024x476.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_creation-300x139.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_creation-768x357.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_creation.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-706977" class="wp-caption-text"&gt;Figure 5: The MortgageApprovalProcess in the Process Instance perspective.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now, log into the Kibana dashboard. We will start with a basic visualization for both processes and tasks. Kibana templates provide an exportable JSON format for sharing graphical reports across instances of Kibana. A sample template &lt;a target="_blank" rel="nofollow" href="https://github.com/snandakumar87/ansible-rhpam-elastic/tree/master/kibana-resources"&gt;can be found here&lt;/a&gt;. Import the template from the Kibana dashboard&amp;#8217;s &lt;strong&gt;Saved Objects&lt;/strong&gt; section, under the &lt;strong&gt;Management&lt;/strong&gt; option.&lt;/p&gt; &lt;p&gt;We now have a sample dashboard available for processes (Figure 6) and tasks (Figure 7).&lt;/p&gt; &lt;div id="attachment_707007" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_dashboard.png"&gt;&lt;img aria-describedby="caption-attachment-707007" class="wp-image-707007 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_dashboard-1024x468.png" alt="Dashboard showing the sample project's processes." width="640" height="293" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_dashboard-1024x468.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_dashboard-300x137.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_dashboard-768x351.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/process_dashboard.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707007" class="wp-caption-text"&gt;Figure 6: The new sample Processes dashboard.&lt;/p&gt;&lt;/div&gt; &lt;div id="attachment_707017" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/task_dashboard.png"&gt;&lt;img aria-describedby="caption-attachment-707017" class="wp-image-707017 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/task_dashboard-1024x488.png" alt="Dashboard showing the sample project's tasks" width="640" height="305" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/task_dashboard-1024x488.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/task_dashboard-300x143.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/task_dashboard-768x366.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/task_dashboard.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707017" class="wp-caption-text"&gt;Figure 7: The new sample Tasks dashboard.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;As these tasks are worked on and the processes head to completion, their status and updates are pushed to Elastic, as shown in Figure 8.&lt;/p&gt; &lt;div id="attachment_707027" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/updates.png"&gt;&lt;img aria-describedby="caption-attachment-707027" class="wp-image-707027 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/updates-1024x497.png" alt="dashboard showing tasks broken down in 4 ways" width="640" height="311" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/updates-1024x497.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/updates-300x146.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/updates-768x372.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/updates.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707027" class="wp-caption-text"&gt;Figure 8: Track the progress of your tasks and processes through their dashboards.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now let us create custom metrics from the process data. We can assume that we need to filter out the cases where property price is over 2 million and property age is less than 25 years. Kibana provides a convenient way to create full-text searches and then lets us convert the result to a visualization.&lt;/p&gt; &lt;p&gt;Open the raw data section of Kibana and create a custom query to filter out the data, as shown in Figure 9.&lt;/p&gt; &lt;div id="attachment_707037" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kibana_querhy.png"&gt;&lt;img aria-describedby="caption-attachment-707037" class="wp-image-707037 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kibana_querhy-1024x403.png" alt="Kibana's raw section with two fields chosen and used in a custom query" width="640" height="252" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/kibana_querhy-1024x403.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kibana_querhy-300x118.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kibana_querhy-768x302.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/kibana_querhy.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707037" class="wp-caption-text"&gt;Figure 9: Create your custom filter.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We can easily convert this data into a visualization by saving this search and creating a bar chart out of it, as shown in Figures 10 and 11.&lt;/p&gt; &lt;div id="attachment_707047" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz.png"&gt;&lt;img aria-describedby="caption-attachment-707047" class="wp-image-707047 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz-1024x482.png" alt="Kibana's New Vertical Bar / Choose a source dialog box." width="640" height="301" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz-1024x482.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz-300x141.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz-768x361.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707047" class="wp-caption-text"&gt;Figure 10: Create a bar chart.&lt;/p&gt;&lt;/div&gt; &lt;div id="attachment_707057" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz_graph.png"&gt;&lt;img aria-describedby="caption-attachment-707057" class="wp-image-707057 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz_graph-1024x511.png" alt="Kibana showing your new bar chart." width="640" height="319" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz_graph-1024x511.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz_graph-300x150.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz_graph-768x383.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/custom_viz_graph.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707057" class="wp-caption-text"&gt;Figure 11: Your new bar chart.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Similar visualizations can be created for cases, too. A sample case visualization using KPI metrics for the case can be set up as shown in Figure 12.&lt;/p&gt; &lt;div id="attachment_707067" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/04/case_viz.png"&gt;&lt;img aria-describedby="caption-attachment-707067" class="wp-image-707067 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/case_viz-1024x485.png" alt="Dashboard showing cases broken down in 4 ways" width="640" height="303" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/case_viz-1024x485.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/case_viz-300x142.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/case_viz-768x364.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/case_viz.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-707067" class="wp-caption-text"&gt;Figure 12: The sample case visualization.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;A complete example of the setup &lt;a target="_blank" rel="nofollow" href="https://github.com/snandakumar87/ansible-rhpam-elastic"&gt;can be found here&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;&lt;b&gt;Summary&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;By setting up an integration with Elasticsearch, we can visualize data from the business automation engine side-by-side with the metrics from other disparate systems. Doing this also provides for faster, more scalable, business-friendly visualizations fit for operations management.&lt;/p&gt; &lt;h2&gt;&lt;b&gt;References&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="http://mswiderski.blogspot.com/2017/08/elasticsearch-empowers-jbpm.html"&gt;Elasticsearch empowers jBPM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-openshift.html"&gt;Deploying Elastic and Kibana on Openshift&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#38;linkname=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F04%2Fmonitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana%2F&amp;#038;title=Monitor%20business%20metrics%20with%20Red%20Hat%20Process%20Automation%20Manager%2C%20Elasticsearch%2C%20and%20Kibana" data-a2a-url="https://developers.redhat.com/blog/2020/05/04/monitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana/" data-a2a-title="Monitor business metrics with Red Hat Process Automation Manager, Elasticsearch, and Kibana"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/04/monitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana/"&gt;Monitor business metrics with Red Hat Process Automation Manager, Elasticsearch, and Kibana&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/BeZggEwPQzQ" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Red Hat Process Automation Manager is a platform for developing containerized microservices and applications that automate business decisions and processes. Combining process- and task-level SLA metrics plus case-related breakdowns can be beneficial for identifying trends and reorganizing the workforce as necessary. So, a critical piece of a business process system is having real-time insights into what [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/04/monitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana/"&gt;Monitor business metrics with Red Hat Process Automation Manager, Elasticsearch, and Kibana&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">706837</post-id><dc:creator>snandaku</dc:creator><dc:date>2020-05-04T07:00:34Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/05/04/monitor-business-metrics-with-red-hat-process-automation-manager-elasticsearch-and-kibana/</feedburner:origLink></entry><entry><title>The Week in JBoss [2020-04-30] - The Virtual Red Hat Summit week</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/NE_f8rJykfA/the-week-in-jboss-2020-04-30-" /><category term="camel-k" scheme="searchisko:content:tags" /><category term="eda" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_weeklyeditorial" scheme="searchisko:content:tags" /><category term="keycloak" scheme="searchisko:content:tags" /><category term="Knative" scheme="searchisko:content:tags" /><category term="kogito" scheme="searchisko:content:tags" /><category term="quarkus" scheme="searchisko:content:tags" /><category term="red hat summit" scheme="searchisko:content:tags" /><author><name>paul.robinson</name></author><id>searchisko:content:id:jbossorg_blog-the_week_in_jboss_2020_04_30_the_virtual_red_hat_summit_week</id><updated>2020-05-01T14:59:18Z</updated><published>2020-05-01T14:59:18Z</published><content type="html">&lt;!-- [DocumentBodyStart:037e93b8-4a46-4673-97d3-4435f15cb221] --&gt;&lt;div class="jive-rendered-content"&gt;&lt;p&gt;&lt;a href="https://www.redhat.com/en/summit" rel="nofollow"&gt;&lt;img alt="" class="image-1 jive-image" height="199" src="https://developer.jboss.org/servlet/JiveServlet/downloadImage/38-6364-439941/Screenshot+2020-05-01+at+15.03.04.png" style="width: 620px; height: 95px;" width="1301"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;I'm writing this as we wrap up another successful Red Hat Summit. But this year, with a significant distinction: the event was 100% virtual. Despite the involuntary move to virtual, there were many benefits that came about from the change in format. The event was completely free, and of course required no travel, allowing a much broader and more diverse set of attendees to benefit from the content and experience. It was also ran in three regions to accommodate many more timezones.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Being Red Hat we were keen to experiment with the format and find new ways to engage with the community. It was important for us to carry over as much of the personality and intimacy of the physical event as possible. So, this needed to be more than just a bunch of streamed talks. The talks were pre-recorded which allowed the presenter(s) to participate directly in the Q&amp;amp;A in real-time as the talk proceeded. There was also a variety of sessions that went beyond the talk format. For examples see &lt;em&gt;Ask the experts&lt;/em&gt;, &lt;em&gt;Networking social hour&lt;/em&gt;, and the &lt;em&gt;Virtual Open Neighborhood&lt;/em&gt; on the &lt;a class="jive-link-external-small" href="https://www.redhat.com/en/summit/agenda/agenda-at-a-glance" rel="nofollow"&gt;agenda&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;If you weren't able to attend, or want to catch some of the talks you missed, you can &lt;a class="jive-link-external-small" href="https://onlinexperiences.com/Launch/QReg.htm?ShowUUID=4245E6E3-7D25-496D-9B08-4CBDC87CCE74" rel="nofollow"&gt;re-live the virtual event here&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;In other news...&lt;/strong&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Quarkus Insights on Youtube Live&lt;/h2&gt;&lt;p&gt;This week Max Andersen and Emmanuel Bernard &lt;a class="jive-link-external-small" href="https://quarkus.io/blog/insights/" rel="nofollow"&gt;kicked off a new video/podcast series&lt;/a&gt; bringing insights into Quarkus. Each episode will focus on a guest speaker discussing the development or usage of Quarkus. There are also some dedicated Q&amp;amp;A sessions planned. Be sure to subscribe to the &lt;a class="jive-link-external-small" href="https://www.youtube.com/c/quarkusio" rel="nofollow"&gt;Quarkus YouTube channel&lt;/a&gt; to catch these sessions and other exciting Quarkus content. In particular &lt;a class="jive-link-external-small" href="https://www.youtube.com/watch?v=OCPFdpvL1Q0&amp;amp;feature=youtu.be" rel="nofollow"&gt;join them on the 4th of May&lt;/a&gt; where Georgios Andrianakis will talk about Quarkus testing and specifically the new mocking improvements in the recently released Quarkus 1.4.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Kogito: A Modular Codegen Design Proposal&lt;/h2&gt;&lt;p&gt;In &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/kogito_a_modular_codegen_design_proposal" rel="nofollow"&gt;this post&lt;/a&gt; Edoardo Vacchi explains how Kogito is improving performance by moving processing out of the run-time and into build-time.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Red Hat Summit 2020 - Ask the Experts: Hybrid Multicloud Pitfalls&lt;/h2&gt;&lt;p&gt;In one of the many &lt;em&gt;Ask the Experts&lt;/em&gt; sessions, Eric Schabell &amp;amp; Roel Hodzelmans focused on their hybrid multi-cloud pitfall theories. You can &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/red_hat_summit_2020_ask_the_experts_hybrid_multicloud_pitfalls_slides" rel="nofollow"&gt;view the slides here&lt;/a&gt;, or &lt;a class="jive-link-external-small" href="https://onlinexperiences.com/Launch/QReg.htm?ShowUUID=4245E6E3-7D25-496D-9B08-4CBDC87CCE74" rel="nofollow"&gt;register for the Red Hat Summit Virtual event&lt;/a&gt; to re-watch the content on demand.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Free book on Knative covering Camel K and Kafka and upcoming webinar with live demos&lt;/h2&gt;&lt;p&gt;In this post Claus Ibsen &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/free_book_on_knative_covering_camel_k_and_kafka_and_upcoming_webinar_with_live_demos" rel="nofollow"&gt;alerts us to the free eBook&lt;/a&gt; written by Burr Sutter &amp;amp; Kamesh Sampath on the subject of Knative. Go get your free copy &lt;a class="jive-link-external-small" href="https://developers.redhat.com/books/knative-cookbook/" rel="nofollow"&gt;here&lt;/a&gt;!&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Red Hat Summit 2020 - Business Automation Sessions&lt;/h2&gt;&lt;p&gt;If you are interested in the area of &lt;em&gt;Business Automation&lt;/em&gt;, be sure to view Kris Verlaenen's &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/virtual_red_hat_summit_2020_april_28_29" rel="nofollow"&gt;helpful summary&lt;/a&gt; of all the BI related talks held at Red Hat Summit.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Demystifying the Event Driven Architecture - An introduction (part 1)&lt;/h2&gt;&lt;p&gt;Eric Schabell has &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/demystifying_the_event_driven_architecture_an_introduction_part_1" rel="nofollow"&gt;started a new blog series&lt;/a&gt; that explores the world of Event Driven Architectures (EDA).&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Six reasons why you will love Camel K&lt;/h2&gt;&lt;p&gt;Interested in Camel K, or want to find out what all the fuss is about? &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/six_reasons_why_you_will_love_camel_k" rel="nofollow"&gt;Read on&lt;/a&gt;, and Christina will give you six reasons to love Camel K.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;Hybrid clouds with JGroups and Skupper&lt;/h2&gt;&lt;p&gt;Bela Ban follows up on his post explaining how to &lt;a class="jive-link-external-small" href="http://belaban.blogspot.com/2019/12/spanning-jgroups-kubernetes-based.html" rel="nofollow"&gt;span JGroups Kubernetes-based clusters across Google and Amazon clouds&lt;/a&gt;. In &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/hybrid_clouds_with_jgroups_and_skupper" rel="nofollow"&gt;this new post&lt;/a&gt; Bela improves on the process by using &lt;a class="jive-link-external-small" href="https://skupper.io/" rel="nofollow"&gt;Skupper&lt;/a&gt; to simplify this task and encrypt the data exchanged between different clouds.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h2&gt;This Week's Releases&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="https://quarkus.io/blog/quarkus-1-4-final-released/" rel="nofollow"&gt;Quarkus 1.4&lt;/a&gt;. Command mode, HTTP 2, New FaaS framework, Mocking, and more.&lt;/li&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="https://www.keycloak.org//2020/04/keycloak-1000-released.html" rel="nofollow"&gt;Keycloak 10.0.0&lt;/a&gt;. With &lt;em&gt;Identity Brokering Sync Mode&lt;/em&gt;, &lt;em&gt;Client Session Timeout for OpenID Connect / OAuth 2.0&lt;/em&gt; and much more.&lt;/li&gt;&lt;li&gt;&lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/kogito_0_9_1_released" rel="nofollow"&gt;Kogito 0.9.1&lt;/a&gt;. This release is a bug fix release, but there has also been considerable work spent on documentation and code examples. See the link for detais.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;!-- [DocumentBodyEnd:037e93b8-4a46-4673-97d3-4435f15cb221] --&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/NE_f8rJykfA" height="1" width="1" alt=""/&gt;</content><summary>  I'm writing this as we wrap up another successful Red Hat Summit. But this year, with a significant distinction: the event was 100% virtual. Despite the involuntary move to virtual, there were many benefits that came about from the change in format. The event was completely free, and of course required no travel, allowing a much broader and more diverse set of attendees to benefit from the conte...</summary><dc:creator>paul.robinson</dc:creator><dc:date>2020-05-01T14:59:18Z</dc:date><feedburner:origLink>https://developer.jboss.org/blogs/weekly-editorial/2020/05/01/the-week-in-jboss-2020-04-30-</feedburner:origLink></entry><entry><title>Automated API testing for the KIE Server</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/4gc8ZtIrGXQ/" /><category term="Containers" /><category term="Developer Tools" /><category term="DevOps" /><category term="Microservices" /><category term="Drools" /><category term="jBPM" /><category term="Red Hat JBoss Enterprise Application Platform" /><category term="Spring Boot" /><category term="test automation" /><author><name>Juliano Mohr</name></author><id>https://developers.redhat.com/blog/?p=702377</id><updated>2020-05-01T07:00:25Z</updated><published>2020-05-01T07:00:25Z</published><content type="html">&lt;p&gt;In software development, much has been said about testing and there are many ways to think about it. In the context of agility and DevOps, automated testing is considered a foundation for the principles of flow and fast feedback. Considering the implementation of &lt;a target="_blank" rel="nofollow" href="https://www.jbpm.org/"&gt;jBPM&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://www.drools.org/"&gt;Drools&lt;/a&gt; within a software delivery project, it becomes natural to think about how to support reliable automated testing for both stages of development and continuous integration.&lt;/p&gt; &lt;p&gt;The KIE Server is a standalone component that can be used to execute business processes and rules via a REST API interface. This article explores the concept of API testing where consumers are expected to interact with the APIs provided by the KIE Server. The test cases consist of HTTP requests and responses exchanged against a live server. Consequently, these test cases are completely agnostic to the technology behind the API, just like real consumers are expected to be.&lt;/p&gt; &lt;h2&gt;Why API testing?&lt;/h2&gt; &lt;p&gt;The API testing approach was previously described as agnostic, but what does that mean? Perhaps it&amp;#8217;s worth explaining by listing the benefits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tests are decoupled from changes in the system under test as long as the API contract is satisfied.&lt;/li&gt; &lt;li&gt;Higher confidence, as tests assert on exact upstream expectations.&lt;/li&gt; &lt;li&gt;Independence, as tests can be maintained as a separate artifact.&lt;/li&gt; &lt;li&gt;Scalability, as tests can be adapted to also assess performance.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;What about unit testing?&lt;/h2&gt; &lt;p&gt;Yes, unit testing is important and should not be neglected. There are well-established techniques in the jBPM and Drools ecosystem to unit test knowledge assets. Such techniques usually comprise of either plain JUnit or the test scenarios offered by &lt;a target="_blank" rel="nofollow" href="http://blog.athico.com/2018/11/workbench-is-now-business-central.html"&gt;Business Central (aka The Workbench)&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;However, there are challenges that unit tests cannot address, especially if JUnit is not an option for your project. In that case, you are limited to only what Business Central offers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Testing rule flows, where the combination of rules fired must be taken into account.&lt;/li&gt; &lt;li&gt;Testing transformations that occur with the input and output of the REST API.&lt;/li&gt; &lt;li&gt;Writing API testing even with no Java or JUnit skills.&lt;/li&gt; &lt;li&gt;Testing API performance (did I mention performance testing already?)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Having said that, it is well known that unit tests are faster and simpler to write, maintain, and troubleshoot. Therefore, one should favor them whenever possible.&lt;/p&gt; &lt;h2&gt;Example implementation with Postman and containers&lt;/h2&gt; &lt;p&gt;If you read along to this point you must be looking for something concrete. Before we get started, Figure 1 illustrates what follows later.&lt;/p&gt; &lt;div id="attachment_702387" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-702387" class="wp-image-702387 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/03/KIE-Server-Postman-Testing-1024x441.png" alt="Automated API Testing for the KIE Server" width="640" height="276" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/03/KIE-Server-Postman-Testing-1024x441.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/KIE-Server-Postman-Testing-300x129.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/KIE-Server-Postman-Testing-768x331.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/03/KIE-Server-Postman-Testing.png 1596w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-702387" class="wp-caption-text"&gt;Figure 1: The example implementation.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;&lt;a href="https://www.postman.com/" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;Postman&lt;/a&gt; is a popular tool for creating and executing HTTP requests. Walking through the diagram, notice that Postman is a core component in this implementation. With it, developers write test cases that are grouped into collections and later exported to a native JSON format. This exported JSON collection is then used as an input to &lt;a href="https://github.com/postmanlabs/newman" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;Newman&lt;/a&gt;, which is a powerful command-line collection runner for Postman. It allows you to run and test a Postman collection directly from the command line.&lt;/p&gt; &lt;p&gt;The system we are testing is &lt;a href="https://developers.redhat.com/products/eap/overview"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; running the KIE Server, which has the &lt;a href="https://developers.redhat.com/blog/2018/03/14/what-is-a-kjar/" target="_blank" rel="noopener noreferrer"&gt;KJAR&lt;/a&gt; deployed.&lt;/p&gt; &lt;p&gt;Finally, both Newman and JBoss / KIE Server are bootstrapped in Linux containers. This setup particularly enables:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fast provisioning of components.&lt;/li&gt; &lt;li&gt;No need to worry about dependencies (NodeJS, Java, etc).&lt;/li&gt; &lt;li&gt;No difference when executed local or in the CI agent (e.g. Jenkins).&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Experiment&lt;/h3&gt; &lt;p&gt;In this example, &lt;code&gt;docker&lt;/code&gt; and &lt;code&gt;docker-compose&lt;/code&gt; are used to simplify the user experience. They are, together with Git, the only pre-requisites to running through the steps:&lt;/p&gt; &lt;pre&gt;$ git clone https://github.com/juliaaano/rhdm-quickstart.git &amp;#38;&amp;#38; cd rhdm-quickstart $ docker-compose up --detach --force-recreate rhdm-jboss&lt;/pre&gt; &lt;p&gt;Check the logs and wait for KIE Server to startup:&lt;/p&gt; &lt;pre&gt;$ docker-compose logs --follow rhdm-jboss&lt;/pre&gt; &lt;p&gt;Run the postman tests:&lt;/p&gt; &lt;pre&gt;$ docker-compose run --rm postman&lt;/pre&gt; &lt;p&gt;Notice it is possible to launch and build the containers directly with other tools such as &lt;a href="https://podman.io/" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;Podman&lt;/a&gt;, &lt;a href="https://buildah.io/" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;Buildah&lt;/a&gt;, and Docker itself.&lt;/p&gt; &lt;p&gt;You can also build the container image locally, which is useful if you made any changes to the source code. This process takes a bit longer, especially in the first run when there is no Docker cache on disk. Additionally, authentication is required with Red Hat&amp;#8217;s container registry to access the &lt;a href="https://access.redhat.com/containers/#/registry.access.redhat.com/rhdm-7/rhdm-kieserver-rhel8" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;base image&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;$ docker login registry.redhat.io $ docker build --file d.jboss.Dockerfile --tag juliaaano/rhdm-jboss .&lt;/pre&gt; &lt;p&gt;Once a new image is built, repeat the step:&lt;/p&gt; &lt;pre&gt;$ docker-compose up --detach --force-recreate rhdm-jboss&lt;/pre&gt; &lt;p&gt;In the &lt;a href="https://github.com/juliaaano/rhdm-quickstart" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;GitHub project you just checked out&lt;/a&gt;, there are instructions for how to run the same app with &lt;a href="https://developers.redhat.com/blog/2018/11/01/spring-boot-enabled-business-process-automation-with-red-hat-process-automation-manager/" target="_blank" rel="noopener noreferrer"&gt;Spring Boot&lt;/a&gt; instead of JBoss EAP. Look over the project&amp;#8217;s README for more information.&lt;/p&gt; &lt;p&gt;The example above features a business rules application, so business processes are left aside. However, the elements demonstrated there should not change and are also applicable to jBPM. KIE-Server and Business Central are integral parts of the &lt;a href="https://developers.redhat.com/products/red-hat-decision-manager/overview/"&gt;Red Hat Decision Manager&lt;/a&gt; (RHDM) and &lt;a href="https://developers.redhat.com/products/rhpam/overview/"&gt;Red Hat Process Automation Manager&lt;/a&gt; (RHPAM) platforms.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F01%2Fautomated-api-testing-for-the-kie-server%2F&amp;#38;linkname=Automated%20API%20testing%20for%20the%20KIE%20Server" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F01%2Fautomated-api-testing-for-the-kie-server%2F&amp;#38;linkname=Automated%20API%20testing%20for%20the%20KIE%20Server" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F01%2Fautomated-api-testing-for-the-kie-server%2F&amp;#38;linkname=Automated%20API%20testing%20for%20the%20KIE%20Server" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F01%2Fautomated-api-testing-for-the-kie-server%2F&amp;#38;linkname=Automated%20API%20testing%20for%20the%20KIE%20Server" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F01%2Fautomated-api-testing-for-the-kie-server%2F&amp;#38;linkname=Automated%20API%20testing%20for%20the%20KIE%20Server" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F01%2Fautomated-api-testing-for-the-kie-server%2F&amp;#38;linkname=Automated%20API%20testing%20for%20the%20KIE%20Server" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F01%2Fautomated-api-testing-for-the-kie-server%2F&amp;#38;linkname=Automated%20API%20testing%20for%20the%20KIE%20Server" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F05%2F01%2Fautomated-api-testing-for-the-kie-server%2F&amp;#038;title=Automated%20API%20testing%20for%20the%20KIE%20Server" data-a2a-url="https://developers.redhat.com/blog/2020/05/01/automated-api-testing-for-the-kie-server/" data-a2a-title="Automated API testing for the KIE Server"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/01/automated-api-testing-for-the-kie-server/"&gt;Automated API testing for the KIE Server&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/4gc8ZtIrGXQ" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In software development, much has been said about testing and there are many ways to think about it. In the context of agility and DevOps, automated testing is considered a foundation for the principles of flow and fast feedback. Considering the implementation of jBPM and Drools within a software delivery project, it becomes natural to [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/05/01/automated-api-testing-for-the-kie-server/"&gt;Automated API testing for the KIE Server&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">702377</post-id><dc:creator>Juliano Mohr</dc:creator><dc:date>2020-05-01T07:00:25Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/05/01/automated-api-testing-for-the-kie-server/</feedburner:origLink></entry><entry><title>Application deployment improvements in OpenShift 4.4</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/hMTFlaSaw-k/" /><category term="Containers" /><category term="Kubernetes" /><category term="Operator" /><category term="Developer Catalog" /><category term="helm 3" /><category term="helm charts" /><category term="openshift 4.4" /><author><name>Serena Chechile Nichols</name></author><id>https://developers.redhat.com/blog/?p=712987</id><updated>2020-04-30T07:03:54Z</updated><published>2020-04-30T07:03:54Z</published><content type="html">&lt;p&gt;The most recent release of &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; provides enhancements and features that make application development even easier. These enhancements include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An enhanced Developer Catalog.&lt;/li&gt; &lt;li&gt;Operator-backed services in the Developer Catalog.&lt;/li&gt; &lt;li&gt;The GA release of Helm 3.&lt;/li&gt; &lt;li&gt;An add context option in the Topology view.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Keep reading to learn more about each of these enhancements in more detail.&lt;/p&gt; &lt;p&gt;&lt;span id="more-712987"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;iframe class='youtube-player' type='text/html' width='640' height='360' src='https://www.youtube.com/embed/elZ2Grd3sgs?version=3&amp;#038;rel=1&amp;#038;fs=1&amp;#038;autohide=2&amp;#038;showsearch=0&amp;#038;showinfo=1&amp;#038;iv_load_policy=1&amp;#038;wmode=transparent' allowfullscreen='true' style='border:0;'&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;h2&gt;Developer Catalog updates&lt;/h2&gt; &lt;p&gt;The enhancements to the Developer Catalog include options to allow developers to filter and group items more easily, and labels to help visually identify the difference between catalog items. These enhancements are aimed at making it easier and faster for developers to find the items they’re looking for, such as filtering the Developer Catalog, grouping the Developer Catalog by Operator, and type badges.&lt;/p&gt; &lt;h3&gt;Filtering the Developer Catalog&lt;/h3&gt; &lt;p&gt;The filtering options in the Developer Catalog have been updated to include the ability to filter by Type, such as Operator Backed, Helm Charts, Builder Image, Template, and Service Class, as shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_713797" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713797" class="wp-image-713797 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F1-FilterByType-1024x536.png" alt="Developer Catalog with Type filters highlighted" width="640" height="335" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F1-FilterByType-1024x536.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F1-FilterByType-300x157.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F1-FilterByType-768x402.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-713797" class="wp-caption-text"&gt;Figure 1: Filtering the Developer Catalog by Type.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;By default, Operator-backed items are selected, but developers can easily change the selections to see the types of items they need.&lt;/p&gt; &lt;h3&gt;Grouping the Developer Catalog by Operator&lt;/h3&gt; &lt;p&gt;When viewing Operator-backed items in the Developer Catalog, you can use the &lt;strong&gt;Group By&lt;/strong&gt; dropdown menu to group items by the Operator that they are associated with, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_713817" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713817" class="wp-image-713817 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F2-CatalogGroupByOperator-1024x537.gif" alt="Developer Catalog grouped by Operator" width="640" height="336" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F2-CatalogGroupByOperator-1024x537.gif 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F2-CatalogGroupByOperator-300x157.gif 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F2-CatalogGroupByOperator-768x403.gif 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-713817" class="wp-caption-text"&gt;Figure 2: Grouping the Developer Catalog by Operator.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;This grouping feature can be useful when an Operator consists of multiple items in the catalog.&lt;/p&gt; &lt;h3&gt;Type badges help to visually differentiate offerings&lt;/h3&gt; &lt;p&gt;To make it easier for developers to scan the Developer Catalog items, the card for each item now contains a label in the top-right corner to indicate the type of the item. For example, in Figure 3, if the user searches for &lt;code&gt;Node&lt;/code&gt; and wants to find the Node.js builder image, the labels make it easier to quickly identify the right item.&lt;/p&gt; &lt;div id="attachment_713807" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713807" class="wp-image-713807 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F3-DevCatalog-TypeBadges-1024x536.png" alt="Developer Catalog making use of type badges" width="640" height="335" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F3-DevCatalog-TypeBadges-1024x536.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F3-DevCatalog-TypeBadges-300x157.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F3-DevCatalog-TypeBadges-768x402.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-713807" class="wp-caption-text"&gt;Figure 3: Type badges help to visually differentiate offerings in the Developer Catalog.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Operator-backed services in the Developer Catalog&lt;/h2&gt; &lt;p&gt;Operator-backed services in the Developer Catalog allow developers to run a variety of workloads that are installed and managed by a Kubernetes Operator. In the Developer Catalog, it’s easy to select and deploy a variety of Operator-backed services.&lt;/p&gt; &lt;h3&gt;Creating Operator-backed services&lt;/h3&gt; &lt;p&gt;In the &lt;b&gt;+Add&lt;/b&gt; navigation section, then select &lt;b&gt;From Catalog&lt;/b&gt; to create an Operator-backed service. By default, the Developer Catalog is filtered to display Operator Backed services. Select the one you want to install. You’ll be brought to the YAML editor, but have no fear, you can click on the &lt;b&gt;Edit Form&lt;/b&gt; link to follow a guided installation process, as shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_713827" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713827" class="wp-image-713827 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F04-1024x588.png" alt="Creating an example Operator-backed Kafka service." width="640" height="368" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F04-1024x588.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F04-300x172.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F04-768x441.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F04.png 1537w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-713827" class="wp-caption-text"&gt;Figure 4: Creating an Operator-backed service.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Operator-backed services in Topology view&lt;/h3&gt; &lt;p&gt;Operator-backed services are visually distinct in Topology view, and the associated resources are grouped together, as shown in Figure 5.&lt;/p&gt; &lt;div id="attachment_713837" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713837" class="wp-image-713837 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F05-1024x580.png" alt="Accessing the resource in the Topology view " width="640" height="363" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F05-1024x580.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F05-300x170.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F05-768x435.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F05.png 1544w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-713837" class="wp-caption-text"&gt;Figure 5: Accessing the resource that manages the Operator-backed service.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;By clicking on the &lt;b&gt;Managed by: &lt;/b&gt;link, you can access the resource that manages the resources that are part of the Operator-backed service, as shown in Figure 6.&lt;/p&gt; &lt;div id="attachment_713847" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713847" class="wp-image-713847 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F06-1024x481.png" alt="Viewing the example's Kafka details" width="640" height="301" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F06-1024x481.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F06-300x141.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F06-768x361.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F06.png 1540w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-713847" class="wp-caption-text"&gt;Figure 6: Viewing the resource that manages the Operator-backed service.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;If you prefer a more streamlined layout in Topology view, you can collapse the information about the Operator-backed services by deselecting the checkbox next to &lt;b&gt;Operator Groupings&lt;/b&gt; in the Display dropdown menu, as shown in Figure 7.&lt;/p&gt; &lt;div id="attachment_713857" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713857" class="wp-image-713857 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F07-1024x471.png" alt="Operator Groupings deselected in the Display drop-down list box" width="640" height="294" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F07-1024x471.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F07-300x138.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F07-768x353.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F07.png 1543w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-713857" class="wp-caption-text"&gt;Figure 7: Collapsed Operator groupings.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Helm 3&lt;/h2&gt; &lt;p&gt;Helm 3 is GA in this release of OpenShift. Developers can now install Helm charts through the Helm CLI as well as the Developer Catalog. Information about Helm releases is displayed in the Developer Perspective.&lt;/p&gt; &lt;p&gt;Helm is a package manager for Kubernetes applications, and it helps define, install, and update apps. Helm uses a packaging format called charts. A Helm Chart is an application package that contains templates for a set of resources (such as a Deployment or Service) that are necessary to run the application. A template uses variables that are substituted with values when the manifest is created. The chart includes a values file that describes how to configure the resources.&lt;/p&gt; &lt;p&gt;With the release of OpenShift 4.4, Helm 3.1 reaches GA. This means that you can find Helm Charts in the Developer Catalog, where developers have the ability to specify custom &lt;code&gt;values.yaml&lt;/code&gt; during chart installation, along with Helm releases in both the &lt;b&gt;Helm&lt;/b&gt; and &lt;b&gt;Topology&lt;/b&gt; navigation sections.&lt;/p&gt; &lt;h3&gt;Installing the Helm 3 CLI&lt;/h3&gt; &lt;p&gt;The Helm 3 CLI is available to download from the OpenShift web console by clicking &lt;strong&gt;Command Line Tools&lt;/strong&gt; under the &lt;strong&gt;Help&lt;/strong&gt; menu, as shown in Figure 8.&lt;/p&gt; &lt;div id="attachment_713867" style="width: 243px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713867" class="wp-image-713867 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F08.png" alt="Help -&amp;#62; Command Line tools" width="233" height="204" /&gt;&lt;p id="caption-attachment-713867" class="wp-caption-text"&gt;Figure 8: Accessing command-line tools&lt;/p&gt;&lt;/div&gt; &lt;p&gt;With the Helm 3 CLI, developers can create and install charts, add repositories to their own local Helm client, and more. You can &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/latest/cli_reference/helm_cli/getting-started-with-helm-on-openshift-container-platform.html"&gt;learn more about the installation and usage of the Helm 3 CLI&lt;/a&gt; in the documentation.&lt;/p&gt; &lt;h3&gt;Installing a Helm Chart in OpenShift&lt;/h3&gt; &lt;p&gt;There are two ways to install Helm charts on OpenShift: using the Helm CLI or from the Developer Catalog.&lt;/p&gt; &lt;p&gt;The Developer Catalog now includes Helm charts from a &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/redhat-helm-charts/"&gt;curated repository&lt;/a&gt;, as shown in Figure 9, allowing the developer to install a chart by customizing the &lt;code&gt;values.yaml&lt;/code&gt;. In a future release of OpenShift, the repository in the Developer Catalog will be configurable.&lt;/p&gt; &lt;div id="attachment_713877" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713877" class="wp-image-713877 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F09-1024x398.png" alt="Developer Catalog with the Helm Charts Type selected and a Helm Chart displayed" width="640" height="249" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F09-1024x398.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F09-300x117.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F09-768x299.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F09.png 1461w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-713877" class="wp-caption-text"&gt;Figure 9: Helm Charts in the Developer Catalog&lt;/p&gt;&lt;/div&gt; &lt;p&gt;To install a Helm chart from the Developer Catalog, filter the catalog items by &lt;b&gt;Type&lt;/b&gt; and select &lt;b&gt;Helm Charts&lt;/b&gt;. When you click a chart to install, you have the opportunity to customize the information in the &lt;code&gt;values.yaml&lt;/code&gt; file, as shown in Figure 10.&lt;/p&gt; &lt;div id="attachment_713887" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713887" class="wp-image-713887 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F10-1024x663.png" alt="Installing a Helm Chart" width="640" height="414" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F10-1024x663.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F10-300x194.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F10-768x497.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F10.png 1455w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-713887" class="wp-caption-text"&gt;Figure 10: Installing a Helm Chart&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Helm Releases in the Developer Perspective&lt;/h3&gt; &lt;p&gt;Information about Helm releases can be found in two places in the Developer Perspective. In Topology view, Helm releases are visible and denoted with the Helm icon, as shown in Figure 11.&lt;/p&gt; &lt;div id="attachment_713897" style="width: 522px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713897" class="wp-image-713897 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F11.png" alt="Viewing Helm Releases in Topology view" width="512" height="334" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F11.png 512w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F11-300x196.png 300w" sizes="(max-width: 512px) 100vw, 512px" /&gt;&lt;p id="caption-attachment-713897" class="wp-caption-text"&gt;Figure 11: Helm Releases in Topology.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Helm releases are also accessible in list format from the left navigation under &lt;b&gt;More -&amp;#62; Helm&lt;/b&gt;, as shown in Figure 12.&lt;/p&gt; &lt;div id="attachment_713907" style="width: 522px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713907" class="wp-image-713907 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F12.png" alt="More -&amp;#62; Helm with Helm Releases displayed" width="512" height="288" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F12.png 512w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F12-300x169.png 300w" sizes="(max-width: 512px) 100vw, 512px" /&gt;&lt;p id="caption-attachment-713907" class="wp-caption-text"&gt;Figure 12: Viewing Helm Releases in a project.&lt;/p&gt;&lt;/div&gt; &lt;h1&gt;Add Context from the Topology view&lt;/h1&gt; &lt;p&gt;Developers can now right-click on the canvas in Topology view to add new resources to applications and projects. When adding items in context, the appropriate service binding is created on the fly, saving the developer time.&lt;/p&gt; &lt;p&gt;Visually laying out the application structure allows developers to make quick decisions and determine better directions. The ability to quickly access the components from the catalog and link them in their application structure in fewer steps can reduce the time and effort required to enhance or extend an application. To enable this feature, the Topology view now allows developers to add a new resource directly to an application or project from the canvas or to create a relevant contextual binding with an existing component seamlessly.&lt;/p&gt; &lt;h2&gt;Accessing the &amp;#8220;Add-in-context&amp;#8221; trigger&lt;/h2&gt; &lt;p&gt;There are two ways to access the Add-in-context trigger: the right-click menu option, and the connector handle.&lt;/p&gt; &lt;h3&gt;Using the right-click menu option&lt;/h3&gt; &lt;p&gt;Performing a right-click anywhere on the canvas displays the contextual actions menu specific to the area that was clicked, as shown in Figure 13. For example, the contextual menu for an application would provide options to add resources to that application, while the contextual menu invoked from the unassigned area on the canvas would display the options to add resources to the project.&lt;/p&gt; &lt;div id="attachment_713917" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713917" class="wp-image-713917" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F13.gif" alt="Animation showing the different contextual menus" width="640" height="352" /&gt;&lt;p id="caption-attachment-713917" class="wp-caption-text"&gt;Figure 13: In-context menus accessed by right-clicking.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Using the connector handle&lt;/h3&gt; &lt;p&gt;On hover, a connector handle extends out of every component. Dragging the arrowhead of the connector handle over any part of the topology graph, except component nodes, morphs it into an &amp;#8220;add&amp;#8221; symbol, as shown in Figure 14.&lt;/p&gt; &lt;div id="attachment_713927" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713927" class="wp-image-713927" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F14.gif" alt="Animation showing the connector handle method" width="640" height="325" /&gt;&lt;p id="caption-attachment-713927" class="wp-caption-text"&gt;Figure 14: In-context menus accessed by the connector handle.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Dropping the cursor over any desired area on the graph invokes a context-specific menu of options to add a resource. Selecting any of those menu options, directs the user to the respective add form, with the context (application and labels) pre-assigned.&lt;/p&gt; &lt;p&gt;Once the action is confirmed, the developer is taken back to the Topology view where they see the newly created component and its associated binding connector, as shown in Figure 15. This single interaction increases efficiency by replacing three actions—creating a resource, assigning an application label to it, and creating a connector to the relevant resource.&lt;/p&gt; &lt;div id="attachment_713937" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713937" class="wp-image-713937 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F15-1024x557.gif" alt="An in-context creation example" width="640" height="348" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F15-1024x557.gif 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F15-300x163.gif 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/F15-768x418.gif 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-713937" class="wp-caption-text"&gt;Figure 15: In-context creation example.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We hope these new features and enhancements will improve your ability to create and deploy applications on OpenShift. In future releases, we plan to provide guided flows by default and decrease the need to edit YAML files.&lt;/p&gt; &lt;h2&gt;Ready to get started?&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="http://www.openshift.com/try"&gt;Try OpenShift today&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Provide your feedback!&lt;/h2&gt; &lt;p&gt;Share &lt;a target="_blank" rel="nofollow" href="https://forms.gle/6HArjszuqyE1xr3f8"&gt;feedback or ideas for application creation and deployment here&lt;/a&gt;. Also, join our &lt;a target="_blank" rel="nofollow" href="https://groups.google.com/forum/#!forum/openshift-dev-users"&gt;OpenShift Developer Experience Google Group&lt;/a&gt; to participate in discussions and learn about our Office Hours session where you can collaborate with us and provide feedback.&lt;/p&gt; &lt;h2&gt;Learn more&lt;/h2&gt; &lt;p&gt;If you’re interested in learning more about application development with OpenShift, &lt;a href="https://developers.redhat.com/blog/2020/04/30/whats-new-in-the-openshift-4-4-web-console-developer-experience/"&gt;learn more&lt;/a&gt; from &lt;a href="https://developers.redhat.com/openshift"&gt;Application development on OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fapplication-deployment-improvements-in-openshift-4-4%2F&amp;#38;linkname=Application%20deployment%20improvements%20in%20OpenShift%204.4" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fapplication-deployment-improvements-in-openshift-4-4%2F&amp;#38;linkname=Application%20deployment%20improvements%20in%20OpenShift%204.4" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fapplication-deployment-improvements-in-openshift-4-4%2F&amp;#38;linkname=Application%20deployment%20improvements%20in%20OpenShift%204.4" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fapplication-deployment-improvements-in-openshift-4-4%2F&amp;#38;linkname=Application%20deployment%20improvements%20in%20OpenShift%204.4" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fapplication-deployment-improvements-in-openshift-4-4%2F&amp;#38;linkname=Application%20deployment%20improvements%20in%20OpenShift%204.4" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fapplication-deployment-improvements-in-openshift-4-4%2F&amp;#38;linkname=Application%20deployment%20improvements%20in%20OpenShift%204.4" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fapplication-deployment-improvements-in-openshift-4-4%2F&amp;#38;linkname=Application%20deployment%20improvements%20in%20OpenShift%204.4" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fapplication-deployment-improvements-in-openshift-4-4%2F&amp;#038;title=Application%20deployment%20improvements%20in%20OpenShift%204.4" data-a2a-url="https://developers.redhat.com/blog/2020/04/30/application-deployment-improvements-in-openshift-4-4/" data-a2a-title="Application deployment improvements in OpenShift 4.4"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/04/30/application-deployment-improvements-in-openshift-4-4/"&gt;Application deployment improvements in OpenShift 4.4&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/hMTFlaSaw-k" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;The most recent release of Red Hat OpenShift Container Platform provides enhancements and features that make application development even easier. These enhancements include: An enhanced Developer Catalog. Operator-backed services in the Developer Catalog. The GA release of Helm 3. An add context option in the Topology view. Keep reading to learn more about each of [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/04/30/application-deployment-improvements-in-openshift-4-4/"&gt;Application deployment improvements in OpenShift 4.4&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">712987</post-id><dc:creator>Serena Chechile Nichols</dc:creator><dc:date>2020-04-30T07:03:54Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/04/30/application-deployment-improvements-in-openshift-4-4/</feedburner:origLink></entry><entry><title>Serverless applications made faster and simpler with OpenShift Serverless GA</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/0dt7CBeTr-Y/" /><category term="CI/CD" /><category term="Event-Driven" /><category term="Kubernetes" /><category term="Serverless" /><category term="containers" /><category term="DevOps" /><category term="Knative" /><category term="microservices" /><category term="openshift" /><author><name>Brian Tannous</name></author><id>https://developers.redhat.com/blog/?p=713047</id><updated>2020-04-30T07:03:34Z</updated><published>2020-04-30T07:03:34Z</published><content type="html">&lt;p&gt;Red Hat OpenShift Serverless delivers Kubernetes-native, event-driven primitives for microservices, containers, and compatible Function-as-a-Service (FaaS) implementations. OpenShft Serverless provides out-of-the-box traffic routing and security capabilities. This offering combines Red Hat Operators, &lt;a target="_blank" rel="nofollow" href="https://knative.dev/"&gt;Knative&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/openshift/"&gt;Red Hat OpenShift&lt;/a&gt;. Combined, these tools allow stateless and serverless workloads to run across OpenShift deployments on private, public, hybrid, or multi-cloud environments with automated operations.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;OpenShift Serverless&lt;/a&gt; is now generally available. It enables developers to focus purely on building next-generation applications with a wide choice of languages, frameworks, development environments, and other tools for writing and deploying business-differentiating applications.&lt;/p&gt; &lt;p&gt;&lt;span id="more-713047"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Key features of Red Hat OpenShift Serverless include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Extensive choice of programming languages and runtimes for serverless applications, letting developers use preferred tools.&lt;/li&gt; &lt;li&gt;Auto-scaling up and down based on requests and events, which helps shift resource consumption based on current rather than legacy needs.&lt;/li&gt; &lt;li&gt;Full integration with OpenShift Pipelines, a Kubernetes-style continuous integration (CI) and continuous delivery (CD) solution that uses Tekton building blocks.&lt;/li&gt; &lt;li&gt;A Red Hat Operator-based foundation, which lets administrators more safely manage and update running instances at scale, and provides a cloud-service-like experience for an application’s lifecycle.&lt;/li&gt; &lt;li&gt;Continued tracking of community release cadence&lt;b&gt;, &lt;/b&gt;including Knative 0.13 Serving, Eventing, and &lt;code&gt;kn&lt;/code&gt;—the official Knative CLI. As with anything we ship as a product at Red Hat, this means we have validated these components on a variety of different platforms and configurations OpenShift runs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We have also engaged with several partners in the Serverless space and have an ongoing collaboration in place with Microsoft around Azure Functions and &lt;a target="_blank" rel="nofollow" href="https://keda.sh/"&gt;KEDA&lt;/a&gt;, and we should have a blog post covering more details about that coming up soon. &lt;a target="_blank" rel="nofollow" href="https://triggermesh.com/"&gt;TriggerMesh&lt;/a&gt; has a certified operator for OpenShift, and most recently &lt;a target="_blank" rel="nofollow" href="http://serverless.com/"&gt;Serverless.com&lt;/a&gt; also decided to partner with us to enable the &lt;a target="_blank" rel="nofollow" href="https://github.com/serverless-components/express-knative"&gt;Serverless Framework&lt;/a&gt; to work with OpenShift Serverless and Knative. These partnerships demonstrate how the serverless space is maturing gradually and with a thriving ecosystem.&lt;/p&gt; &lt;p&gt;While new installations should receive the generally available version, older installations can upgrade smoothly. Administrators with existing deployments of OpenShift Serverless that are using the Technology Preview will need to reconfigure the OLM Subscription Update Channel to receive the update, as shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_714037" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-714037" class="wp-image-714037 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/01-updateSubscriptionChannel-1024x375.png" alt="Change Subscription Update Channel dialog box" width="640" height="234" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/01-updateSubscriptionChannel-1024x375.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/01-updateSubscriptionChannel-300x110.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/01-updateSubscriptionChannel-768x282.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/01-updateSubscriptionChannel.png 1200w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-714037" class="wp-caption-text"&gt;Figure 1: Update the subscription channel.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The subscription update channel should be updated to match the OpenShift Container Platform Version of either 4.4 or 4.3.&lt;/p&gt; &lt;h2&gt;Knative Services as first-class citizens&lt;/h2&gt; &lt;p&gt;OpenShift 4.4 makes it easier than ever to deploy applications using OpenShift Serverless by providing you a simplified way to deploy Knative Services within the Developer perspective of the OpenShift Web Console.&lt;/p&gt; &lt;p&gt;Now, you can select a Knative Service Resource Type when adding a new application into the project, instantly allowing any application to benefit from the power that is OpenShift Serverless—including the ability to scale to zero when idle, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_714017" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-714017" class="wp-image-714017 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F02.gif" alt="" width="640" height="395" /&gt;&lt;p id="caption-attachment-714017" class="wp-caption-text"&gt;Figure 2: Choose a Knative Service resource type.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Simplified installation experience with Kourier&lt;/h2&gt; &lt;p&gt;As mentioned in &lt;em&gt;&lt;a target="_blank" rel="nofollow" href="https://www.openshift.com/blog/announcing-openshift-serverless-1-5-0-tech-preview-a-sneak-peek-of-our-ga"&gt;Announcing OpenShift Serverless 1.5.0 Tech Preview – A sneak peek of our GA&lt;/a&gt;&lt;/em&gt;, the most recent versions of OpenShift Serverless use &lt;a target="_blank" rel="nofollow" href="https://github.com/knative/net-kourier"&gt;Kourier&lt;/a&gt;. By using Kourier, we can bring the list of requirements to get Serverless installed in OpenShift down even smaller than our Technology Preview releases. This factor allows for lower resource consumption and faster running application cold starts. We also avoid impacting non-serverless workloads running on the same namespace.&lt;/p&gt; &lt;p&gt;Overall these improvements in combination with fixes we implemented in OpenShift 4.3.5 speed up the time to create an application from a pre-built container between 40–50%, depending on the container image size.&lt;/p&gt; &lt;p&gt;Figure 3 shows the results before using Kourier.&lt;/p&gt; &lt;div id="attachment_714007" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-714007" class="wp-image-714007 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F03.gif" alt="output of kn service create before installing Kourier" width="640" height="205" /&gt;&lt;p id="caption-attachment-714007" class="wp-caption-text"&gt;Figure 3: Create time before Kourier.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 4 shows the create time after adding Kourier:&lt;/p&gt; &lt;div id="attachment_713997" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713997" class="wp-image-713997" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/04-afterKourier.gif" alt="output of kn service create after installing Kourier" width="640" height="205" /&gt;&lt;p id="caption-attachment-713997" class="wp-caption-text"&gt;Figure 4: Create time after Kourier.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Automatic TLS/SSL&lt;/h2&gt; &lt;p&gt;OpenShift Serverless now allows for automatic creation and deployment of TLS/SSL for a Knative Service&amp;#8217;s given route. This means that you can continue focusing on developing your application instead of running and managing it. As a result, Serverless can handle the complexities of managing the deployment while still maintaining the security developers expect from Red Hat OpenShift.&lt;/p&gt; &lt;h2&gt;OpenShift Serverless command-line interface (CLI)&lt;/h2&gt; &lt;p&gt;The OpenShift Serverless command-line interface (CLI)  &lt;code&gt;kn&lt;/code&gt; is now available via the OpenShift Console&amp;#8217;s Command Line Tools section, as shown in Figure 5.&lt;/p&gt; &lt;div id="attachment_713987" style="width: 649px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713987" class="wp-image-713987" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/05-downloadKnCli.png" alt="OpenShift Serverless Command Line Interface download page" width="639" height="422" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/05-downloadKnCli.png 1000w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/05-downloadKnCli-300x198.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/05-downloadKnCli-768x507.png 768w" sizes="(max-width: 639px) 100vw, 639px" /&gt;&lt;p id="caption-attachment-713987" class="wp-caption-text"&gt;Figure 5: Download the OpenShift Serverless command-line interface.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;When using this interface to download the serverless CLI, enterprise customers have access to a Red Hat-notarized version of &lt;code&gt;kn&lt;/code&gt;. MacOS, Windows, or Linux users can have more confidence that this distributed CLI has been checked for malicious components.&lt;/p&gt; &lt;p&gt;In Figure 6, we use &lt;code&gt;kn&lt;/code&gt; to deploy a service in a single command, instantiating our application on OpenShift with a URL to access it within a few seconds.&lt;/p&gt; &lt;div id="attachment_713977" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713977" class="wp-image-713977 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F06.gif" alt="the kn CLI in action" width="640" height="257" /&gt;&lt;p id="caption-attachment-713977" class="wp-caption-text"&gt;Figure 6: Using the &lt;code&gt;kn&lt;/code&gt; CLI.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;This tool allows you to fully manage both Serverless Serving and Eventing resources without the need to edit or view any YAML configurations.&lt;/p&gt; &lt;h2&gt;Developer Topology enhancements&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s take a look at the Topology view enhancements to make managing OpenShift Knative Services easier.&lt;/p&gt; &lt;h3&gt;Knative Service-centric visualizations&lt;/h3&gt; &lt;p&gt;Knative Services are represented as a rectangle containing all of the revisions within the &lt;strong&gt;Topology&lt;/strong&gt; view, as shown in Figure 7.&lt;/p&gt; &lt;div id="attachment_713967" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713967" class="wp-image-713967 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F07.gif" alt="OpenShift Topology view showing Knative Services" width="640" height="395" /&gt;&lt;p id="caption-attachment-713967" class="wp-caption-text"&gt;Figure 7: Knative Services in the Topology view.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;You can quickly see the current traffic distribution percentages for a Knative Service. It is also possible to group Knative Services within an application group, which allows for quick and organized visual management of what is happening within a given grouping.&lt;/p&gt; &lt;h3&gt;Collapsing OpenShift Knative Services&lt;/h3&gt; &lt;p&gt;Speaking of application grouping, OpenShift 4.4 brings the ability to collapse Knative Services within an application group. This makes it easier to manage and view your services when more complex applications are deployed within a project.&lt;/p&gt; &lt;h3&gt;Knative Service details&lt;/h3&gt; &lt;p&gt;OpenShift 4.4 also provides improved side panel content for Knative Services. The &lt;b&gt;Resources&lt;/b&gt; tab within the side panel now shows the components that the service is composed of, specifically: Pods, Revisions, and Routes. You can also use these components for quick and easy access to individual pod logs.&lt;/p&gt; &lt;p&gt;This view shows the traffic distribution percentages and even allows for a rapid configuration change. As a result, you can quickly notice the live traffic distribution for the given Knative Service via the number of pods that are running for a given revision, as shown in Figure 8.&lt;/p&gt; &lt;div id="attachment_713957" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713957" class="wp-image-713957 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/F08.gif" alt="side panel showing Knative Service traffic distribution" width="640" height="395" /&gt;&lt;p id="caption-attachment-713957" class="wp-caption-text"&gt;Figure 8: View Knative Service traffic distribution.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;A deeper view into Revisions&lt;/h3&gt; &lt;p&gt;The Developer Topology view, for now, includes a much deeper view into individual Serverless Revisions. Now, you can quickly see all pods for a revision and view their logs if needed. This view also allows for easy access to the Revision’s Deployments and Configurations, as well as a sub-route that points directly to the given revision, as shown in Figure 9.&lt;/p&gt; &lt;div id="attachment_713947" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-713947" class="wp-image-713947 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/04/09-revSlideOut-1024x758.png" alt="Developer -&amp;#62; Topology showing the resources associated with revisions" width="640" height="474" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/04/09-revSlideOut-1024x758.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/09-revSlideOut-300x222.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/09-revSlideOut-768x568.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/04/09-revSlideOut.png 1138w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-713947" class="wp-caption-text"&gt;Figure 9: Resources associated with Revisions.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We hope these features help you successfully create and manage serverless applications. In future releases, we’ll provide even more functionality in the developer experience. Keep an eye out for the ability to create event sources, and more!&lt;/p&gt; &lt;h2&gt;Ready to get started?&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="http://www.openshift.com/try"&gt;Try OpenShift today.&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Provide your feedback&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://forms.gle/6HArjszuqyE1xr3f8"&gt;Let us know what you think&lt;/a&gt; about the serverless experience. Keep the feedback coming, your feedback allows us to help you. Also, you can join our &lt;a target="_blank" rel="nofollow" href="https://groups.google.com/forum/#!forum/openshift-dev-users"&gt;OpenShift Developer Experience Google Group&lt;/a&gt; to participate in discussions and learn about our Office Hours session, where you can collaborate with us and provide feedback.&lt;/p&gt; &lt;h2&gt;Learn more&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2020/04/30/whats-new-in-the-openshift-4-4-web-console-developer-experience/"&gt;Learn more&lt;/a&gt; about OpenShift application development with these Red Hat resources:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://www.openshift.com/serverless"&gt;OpenShift Serverless resources&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;OpenShift Serverless and Knative&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://redhat-developer-demos.github.io/knative-tutorial/knative-tutorial/index.html"&gt;Knative Tutorial&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/openshift"&gt;Application development on OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://learn.openshift.com/middleware/serverless/"&gt;Introduction to OpenShift Serverless Tutorialå&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fserverless-applications-made-faster-and-simpler-with-openshift-serverless-ga%2F&amp;#38;linkname=Serverless%20applications%20made%20faster%20and%20simpler%20with%20OpenShift%20Serverless%20GA" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fserverless-applications-made-faster-and-simpler-with-openshift-serverless-ga%2F&amp;#38;linkname=Serverless%20applications%20made%20faster%20and%20simpler%20with%20OpenShift%20Serverless%20GA" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fserverless-applications-made-faster-and-simpler-with-openshift-serverless-ga%2F&amp;#38;linkname=Serverless%20applications%20made%20faster%20and%20simpler%20with%20OpenShift%20Serverless%20GA" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fserverless-applications-made-faster-and-simpler-with-openshift-serverless-ga%2F&amp;#38;linkname=Serverless%20applications%20made%20faster%20and%20simpler%20with%20OpenShift%20Serverless%20GA" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fserverless-applications-made-faster-and-simpler-with-openshift-serverless-ga%2F&amp;#38;linkname=Serverless%20applications%20made%20faster%20and%20simpler%20with%20OpenShift%20Serverless%20GA" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fserverless-applications-made-faster-and-simpler-with-openshift-serverless-ga%2F&amp;#38;linkname=Serverless%20applications%20made%20faster%20and%20simpler%20with%20OpenShift%20Serverless%20GA" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fserverless-applications-made-faster-and-simpler-with-openshift-serverless-ga%2F&amp;#38;linkname=Serverless%20applications%20made%20faster%20and%20simpler%20with%20OpenShift%20Serverless%20GA" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F04%2F30%2Fserverless-applications-made-faster-and-simpler-with-openshift-serverless-ga%2F&amp;#038;title=Serverless%20applications%20made%20faster%20and%20simpler%20with%20OpenShift%20Serverless%20GA" data-a2a-url="https://developers.redhat.com/blog/2020/04/30/serverless-applications-made-faster-and-simpler-with-openshift-serverless-ga/" data-a2a-title="Serverless applications made faster and simpler with OpenShift Serverless GA"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/04/30/serverless-applications-made-faster-and-simpler-with-openshift-serverless-ga/"&gt;Serverless applications made faster and simpler with OpenShift Serverless GA&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/0dt7CBeTr-Y" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Red Hat OpenShift Serverless delivers Kubernetes-native, event-driven primitives for microservices, containers, and compatible Function-as-a-Service (FaaS) implementations. OpenShft Serverless provides out-of-the-box traffic routing and security capabilities. This offering combines Red Hat Operators, Knative, and Red Hat OpenShift. Combined, these tools allow stateless and serverless workloads to run across OpenShift deployments on private, public, hybrid, or multi-cloud [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/04/30/serverless-applications-made-faster-and-simpler-with-openshift-serverless-ga/"&gt;Serverless applications made faster and simpler with OpenShift Serverless GA&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">713047</post-id><dc:creator>Brian Tannous</dc:creator><dc:date>2020-04-30T07:03:34Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/04/30/serverless-applications-made-faster-and-simpler-with-openshift-serverless-ga/</feedburner:origLink></entry></feed>
